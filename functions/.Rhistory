x <- sort(na.omit(samples))
n <- length(x)
n1 <- n - 1
return(atanh(cor(x, ((sum(x^2) - x^2)/n1 - ((sum(x) - x)/n1)^2)^(1/3))))
}
# Vasicek K_{m,n} statistic
calculate_vasicek_kmn <- function(samples) {
m = floor(sqrt(sum(!is.na(samples))))
x <- sort(na.omit(samples))
n <- length(x)
s <- sqrt(mean((x - mean(x))^2))
return((n/(2*m*s)) * exp(mean(log(x[pmin(n, 1:n + m)] - x[pmax(1L, 1:n - m)]))))
}
# ----------------------------------
# Safe Feature Calculation Wrapper
# ----------------------------------
safe_calculate <- function(expr, default = NA) {
tryCatch(expr, error = function(e) default)
}
# ----------------------------
#  calculate features
# ----------------------------
calculate_features <- function(samples) {
# Remove NA values
samples <- na.omit(samples)
# Basic statistics
mean_val <- mean(samples)
median_val <- median(samples)
var_val <- var(samples)
iqr_val <- IQR(samples)
mad_val <- mad(samples)
range_val <- max(samples) - min(samples)
cv_val <- if(mean_val != 0) sd(samples)/mean_val else NA
rms_val <- sqrt(mean(samples^2))
# Shape statistics
skewness <- safe_calculate(e1071::skewness(samples))
kurtosis <- safe_calculate(e1071::kurtosis(samples))
# Normality tests
jb_stat <- safe_calculate(tseries::jarque.bera.test(samples)$statistic)
ad_stat <- safe_calculate(nortest::ad.test(samples)$statistic)
sw_stat <- safe_calculate(shapiro.test(samples)$statistic)
sf_stat <- safe_calculate(nortest::sf.test(samples)$statistic)
lf_stat <- safe_calculate(nortest::lillie.test(samples)$statistic)
# Additional features
zcr <- safe_calculate(calculate_zero_crossing_rate(samples))
gini <- safe_calculate(ineq::ineq(abs(samples - min(samples)), type = "Gini"))
outliers <- safe_calculate(calculate_outliers(samples))
pt_ratio <- safe_calculate(calculate_peak_to_trough(samples))
box_val <- safe_calculate(Box.test(samples, lag = 1, type = "Ljung-Box")$statistic)
spec_entropy <- safe_calculate(calculate_spectral_entropy(samples))
spec_centroid <- safe_calculate(calculate_spectral_centroid(samples))
fd_val <- safe_calculate(calculate_fractal_dimension(samples))
hjorth_vals <- safe_calculate(calculate_hjorth(samples))
energy <- sum(samples^2)
# Zp and Vasicek’s statistic
zp_stat <- safe_calculate(calculate_zp_statistic(samples))
vas_kmn <- safe_calculate(calculate_vasicek_kmn(samples))
# Statistical features
Tail_Weight_Ratio <- safe_calculate(quantile(samples, 0.95) / quantile(samples, 0.05))
Moment_Ratio <- safe_calculate(moments::moment(samples, 4) / (moments::moment(samples, 2)^2))
# Create feature data frame
features <- data.frame(
Median = median_val,
Variance = var_val,
IQR = iqr_val,
MAD = mad_val,
Range = range_val,
CV = cv_val,
Root_Mean_Square = rms_val,
Skewness = skewness,
Kurtosis = kurtosis,
Jarque_Bera = jb_stat,
Anderson_Darling = ad_stat,
Shapiro_Wilk = sw_stat,
Shapiro_Francia = sf_stat,
Lilliefors = lf_stat,
Zero_Cross_Rate = zcr,
Gini_Coefficient = gini,
Outliers = outliers,
Peak_to_Trough = pt_ratio,
Spectral_Entropy = spec_entropy,
Energy = energy,
Tail_Weight_Ratio = Tail_Weight_Ratio,
Moment_Ratio = Moment_Ratio,
Zp = zp_stat,
Vasicek_Kmn = vasicek_stat,
stringsAsFactors = FALSE
)
return(features)
}
set.seed(1)
x_norm  <- gen_data(200, "normal")
x_gamma <- gen_data(200, "gamma")
calculate_zp_statistic(x_norm)
calculate_zp_statistic(x_gamma)
set.seed(3)
x <- gen_data(300, "normal")
calculate_features(x)
# -----------------------------------
#         INSTALL PACKAGES          #
# -----------------------------------
if (!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest, lawstat, moments, ineq, caret,
randomForest, gbm, pROC, ROCR, nnet, ggplot2, mlbench, dplyr, kernlab,fractaldim
)
# -----------------------------------
# 1.  DATA GENERATION FUNCTION      #
# -----------------------------------
gen_data <- function(n, dist, par = NULL) {
# Input validation
if (!is.numeric(n) || n <= 0) stop("n must be a positive integer")
dist <- tolower(dist)
if (dist == "normal") {
if (is.null(par)) par <- c(0, 1)
samples <- rnorm(n, mean = par[1], sd = par[2])
} else if (dist == "chi_square") {
if (is.null(par)) par <- 3
samples <- rchisq(n, df = par)
}else if (dist == "gamma") {
if (is.null(par)) par <- c(3, 0.1)
samples <- rgamma(n, shape = par[1], rate = par[2])
} else if (dist == "exponential") {
if (is.null(par)) par <- 1
samples <- rexp(n, rate = par)
} else if (dist == "f") {
if (is.null(par)) par <- c(6, 15)
samples <- rf(n, df1 = par[1], df2 = par[2])
} else if (dist == "t") {
if (is.null(par)) par <- 3
samples <- rt(n, df = par)
}else if (dist == "uniform") {
if (is.null(par)) par <- c(0, 1)
samples <- runif(n, min = par[1], max = par[2])
} else if (dist == "laplace") {
if (is.null(par)) par <- c(2, 7)
samples <- LaplacesDemon::rlaplace(n, location = par[1], scale = par[2])
} else if (dist == "weibull") {
if (is.null(par)) par <- c(1, 2)
samples <- rweibull(n, shape = par[1], scale = par[2])
} else if (dist == "lognormal") {
if (is.null(par)) par <- c(0, 1)
samples <- rlnorm(n, meanlog = par[1], sdlog = par[2])
} else if (dist == "contaminated") {
if (is.null(par)) par <- c(0.75, 0, 1, 5)
br <- rbinom(n, size = 1, prob = par[1])
sd_br <- ifelse(br == 1, par[3], par[4])
samples <- rnorm(n, mean = par[2], sd = sd_br)
} else if (dist == "pareto") {
if (is.null(par)) par <- c(1, 3)
samples <- VGAM::rpareto(n, scale = par[1], shape = par[2])
}else if (dist == "beta") {
if (is.null(par)) par <- c(2, 5)
if (any(par <= 0)) stop("Beta parameters must be > 0")
samples <- rbeta(n, shape1 = par[1], shape2 = par[2])
}else if (dist == "logistic") {
if (is.null(par)) par <- c(0, 1)
if (par[2] <= 0) stop("Logistic scale must be > 0")
samples <- rlogis(n, location = par[1], scale = par[2])
} else {
stop("Unsupported distribution: ", dist)
}
return(samples)
}
# ========================================================================
# -----------------------------------
# 2. FEATURE EXTRACTION FUNCTIONS
# -----------------------------------
# Zero Crossing Rate
calculate_zero_crossing_rate <- function(samples) {
signs <- samples > 0
zero_crossings <- sum(abs(diff(signs)))
return(zero_crossings / (length(samples) - 1))
}
# Outlier Detection
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
return(sum(samples < (qnt[1] - H) | samples > (qnt[2] + H)))
}
# Peak to Trough Ratio
calculate_peak_to_trough <- function(samples) {
max_val <- max(samples)
min_val <- min(samples)
if(min_val == 0) return(NA)
return(max_val / abs(min_val))
}
# Spectral Entropy
calculate_spectral_entropy <- function(samples) {
spec <- spec.pgram(samples, plot = FALSE, na.action = na.omit)
p <- spec$spec / sum(spec$spec)
p <- p[p > 0]
return(-sum(p * log(p)))
}
# Lin & Mudholkar Zp statistic
calculate_zp_statistic <- function(samples) {
x <- sort(na.omit(samples))
n <- length(x)
n1 <- n - 1
return(atanh(cor(x, ((sum(x^2) - x^2)/n1 - ((sum(x) - x)/n1)^2)^(1/3))))
}
# Vasicek K_{m,n} statistic
calculate_vasicek_kmn <- function(samples) {
m = floor(sqrt(sum(!is.na(samples))))
x <- sort(na.omit(samples))
n <- length(x)
s <- sqrt(mean((x - mean(x))^2))
return((n/(2*m*s)) * exp(mean(log(x[pmin(n, 1:n + m)] - x[pmax(1L, 1:n - m)]))))
}
# ----------------------------------
# Safe Feature Calculation Wrapper
# ----------------------------------
safe_calculate <- function(expr, default = NA) {
tryCatch(expr, error = function(e) default)
}
# ----------------------------
#  calculate features
# ----------------------------
calculate_features <- function(samples) {
# Remove NA values
samples <- na.omit(samples)
# Basic statistics
mean_val <- mean(samples)
median_val <- median(samples)
var_val <- var(samples)
iqr_val <- IQR(samples)
mad_val <- mad(samples)
range_val <- max(samples) - min(samples)
cv_val <- if(mean_val != 0) sd(samples)/mean_val else NA
rms_val <- sqrt(mean(samples^2))
# Shape statistics
skewness <- safe_calculate(e1071::skewness(samples))
kurtosis <- safe_calculate(e1071::kurtosis(samples))
# Normality tests
jb_stat <- safe_calculate(tseries::jarque.bera.test(samples)$statistic)
ad_stat <- safe_calculate(nortest::ad.test(samples)$statistic)
sw_stat <- safe_calculate(shapiro.test(samples)$statistic)
sf_stat <- safe_calculate(nortest::sf.test(samples)$statistic)
lf_stat <- safe_calculate(nortest::lillie.test(samples)$statistic)
# Additional features
zcr <- safe_calculate(calculate_zero_crossing_rate(samples))
gini <- safe_calculate(ineq::ineq(abs(samples - min(samples)), type = "Gini"))
outliers <- safe_calculate(calculate_outliers(samples))
pt_ratio <- safe_calculate(calculate_peak_to_trough(samples))
box_val <- safe_calculate(Box.test(samples, lag = 1, type = "Ljung-Box")$statistic)
spec_entropy <- safe_calculate(calculate_spectral_entropy(samples))
spec_centroid <- safe_calculate(calculate_spectral_centroid(samples))
fd_val <- safe_calculate(calculate_fractal_dimension(samples))
hjorth_vals <- safe_calculate(calculate_hjorth(samples))
energy <- sum(samples^2)
# Zp and Vasicek’s statistic
zp_stat <- safe_calculate(calculate_zp_statistic(samples))
vasicek_stat <- safe_calculate(calculate_vasicek_kmn(samples))
# Statistical features
Tail_Weight_Ratio <- safe_calculate(quantile(samples, 0.95) / quantile(samples, 0.05))
Moment_Ratio <- safe_calculate(moments::moment(samples, 4) / (moments::moment(samples, 2)^2))
# Create feature data frame
features <- data.frame(
Median = median_val,
Variance = var_val,
IQR = iqr_val,
MAD = mad_val,
Range = range_val,
CV = cv_val,
Root_Mean_Square = rms_val,
Skewness = skewness,
Kurtosis = kurtosis,
Jarque_Bera = jb_stat,
Anderson_Darling = ad_stat,
Shapiro_Wilk = sw_stat,
Shapiro_Francia = sf_stat,
Lilliefors = lf_stat,
Zero_Cross_Rate = zcr,
Gini_Coefficient = gini,
Outliers = outliers,
Peak_to_Trough = pt_ratio,
Spectral_Entropy = spec_entropy,
Energy = energy,
Tail_Weight_Ratio = Tail_Weight_Ratio,
Moment_Ratio = Moment_Ratio,
Zp = zp_stat,
Vasicek_Kmn = vasicek_stat,
stringsAsFactors = FALSE
)
return(features)
}
set.seed(1)
x_norm  <- gen_data(200, "normal")
x_gamma <- gen_data(200, "gamma")
calculate_zp_statistic(x_norm)
calculate_zp_statistic(x_gamma)
set.seed(3)
x <- gen_data(300, "normal")
calculate_features(x)
calculate_features(x)
# -----------------------------------
#         INSTALL PACKAGES          #
# -----------------------------------
if (!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest, lawstat, moments, ineq, caret,
randomForest, gbm, pROC, ROCR, nnet, ggplot2, mlbench, dplyr, kernlab,fractaldim
)
# -----------------------------------
# 1.  DATA GENERATION FUNCTION      #
# -----------------------------------
gen_data <- function(n, dist, par = NULL) {
# Input validation
if (!is.numeric(n) || n <= 0) stop("n must be a positive integer")
dist <- tolower(dist)
if (dist == "normal") {
if (is.null(par)) par <- c(0, 1)
samples <- rnorm(n, mean = par[1], sd = par[2])
} else if (dist == "chi_square") {
if (is.null(par)) par <- 3
samples <- rchisq(n, df = par)
}else if (dist == "gamma") {
if (is.null(par)) par <- c(3, 0.1)
samples <- rgamma(n, shape = par[1], rate = par[2])
} else if (dist == "exponential") {
if (is.null(par)) par <- 1
samples <- rexp(n, rate = par)
} else if (dist == "f") {
if (is.null(par)) par <- c(6, 15)
samples <- rf(n, df1 = par[1], df2 = par[2])
} else if (dist == "t") {
if (is.null(par)) par <- 3
samples <- rt(n, df = par)
}else if (dist == "uniform") {
if (is.null(par)) par <- c(0, 1)
samples <- runif(n, min = par[1], max = par[2])
} else if (dist == "laplace") {
if (is.null(par)) par <- c(2, 7)
samples <- LaplacesDemon::rlaplace(n, location = par[1], scale = par[2])
} else if (dist == "weibull") {
if (is.null(par)) par <- c(1, 2)
samples <- rweibull(n, shape = par[1], scale = par[2])
} else if (dist == "lognormal") {
if (is.null(par)) par <- c(0, 1)
samples <- rlnorm(n, meanlog = par[1], sdlog = par[2])
} else if (dist == "contaminated") {
if (is.null(par)) par <- c(0.75, 0, 1, 5)
br <- rbinom(n, size = 1, prob = par[1])
sd_br <- ifelse(br == 1, par[3], par[4])
samples <- rnorm(n, mean = par[2], sd = sd_br)
} else if (dist == "pareto") {
if (is.null(par)) par <- c(1, 3)
samples <- VGAM::rpareto(n, scale = par[1], shape = par[2])
}else if (dist == "beta") {
if (is.null(par)) par <- c(2, 5)
if (any(par <= 0)) stop("Beta parameters must be > 0")
samples <- rbeta(n, shape1 = par[1], shape2 = par[2])
}else if (dist == "logistic") {
if (is.null(par)) par <- c(0, 1)
if (par[2] <= 0) stop("Logistic scale must be > 0")
samples <- rlogis(n, location = par[1], scale = par[2])
} else {
stop("Unsupported distribution: ", dist)
}
return(samples)
}
# ========================================================================
# -----------------------------------
# 2. FEATURE EXTRACTION FUNCTIONS
# -----------------------------------
# Zero Crossing Rate
calculate_zero_crossing_rate <- function(samples) {
signs <- samples > 0
zero_crossings <- sum(abs(diff(signs)))
return(zero_crossings / (length(samples) - 1))
}
# Outlier Detection
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
return(sum(samples < (qnt[1] - H) | samples > (qnt[2] + H)))
}
# Peak to Trough Ratio
calculate_peak_to_trough <- function(samples) {
max_val <- max(samples)
min_val <- min(samples)
if(min_val == 0) return(NA)
return(max_val / abs(min_val))
}
# Spectral Entropy
calculate_spectral_entropy <- function(samples) {
spec <- spec.pgram(samples, plot = FALSE, na.action = na.omit)
p <- spec$spec / sum(spec$spec)
p <- p[p > 0]
return(-sum(p * log(p)))
}
# Lin & Mudholkar Zp statistic
calculate_zp_statistic <- function(samples) {
x <- sort(na.omit(samples))
n <- length(x)
n1 <- n - 1
return(atanh(cor(x, ((sum(x^2) - x^2)/n1 - ((sum(x) - x)/n1)^2)^(1/3))))
}
# Vasicek K_{m,n} statistic
calculate_vasicek_kmn <- function(samples) {
m = floor(sqrt(sum(!is.na(samples))))
x <- sort(na.omit(samples))
n <- length(x)
s <- sqrt(mean((x - mean(x))^2))
return((n/(2*m*s)) * exp(mean(log(x[pmin(n, 1:n + m)] - x[pmax(1L, 1:n - m)]))))
}
# ----------------------------------
# Safe Feature Calculation Wrapper
# ----------------------------------
safe_calculate <- function(expr, default = NA) {
tryCatch(expr, error = function(e) default)
}
# ----------------------------
#  calculate features
# ----------------------------
calculate_features <- function(samples) {
# Remove NA values
samples <- na.omit(samples)
# Basic statistics
mean_val <- mean(samples)
median_val <- median(samples)
var_val <- var(samples)
iqr_val <- IQR(samples)
mad_val <- mad(samples)
range_val <- max(samples) - min(samples)
cv_val <- if(mean_val != 0) sd(samples)/mean_val else NA
rms_val <- sqrt(mean(samples^2))
# Shape statistics
skewness <- safe_calculate(e1071::skewness(samples))
kurtosis <- safe_calculate(e1071::kurtosis(samples))
# Normality tests
jb_stat <- safe_calculate(tseries::jarque.bera.test(samples)$statistic)
ad_stat <- safe_calculate(nortest::ad.test(samples)$statistic)
sw_stat <- safe_calculate(shapiro.test(samples)$statistic)
sf_stat <- safe_calculate(nortest::sf.test(samples)$statistic)
lf_stat <- safe_calculate(nortest::lillie.test(samples)$statistic)
# Additional features
zcr <- safe_calculate(calculate_zero_crossing_rate(samples))
gini <- safe_calculate(ineq::ineq(abs(samples - min(samples)), type = "Gini"))
outliers <- safe_calculate(calculate_outliers(samples))
pt_ratio <- safe_calculate(calculate_peak_to_trough(samples))
box_val <- safe_calculate(Box.test(samples, lag = 1, type = "Ljung-Box")$statistic)
spec_entropy <- safe_calculate(calculate_spectral_entropy(samples))
spec_centroid <- safe_calculate(calculate_spectral_centroid(samples))
fd_val <- safe_calculate(calculate_fractal_dimension(samples))
hjorth_vals <- safe_calculate(calculate_hjorth(samples))
energy <- sum(samples^2)
# Zp and Vasicek’s statistic
zp_stat <- safe_calculate(calculate_zp_statistic(samples))
vasicek_stat <- safe_calculate(calculate_vasicek_kmn(samples))
# Statistical features
Tail_Weight_Ratio <- safe_calculate(quantile(samples, 0.95) / quantile(samples, 0.05))
Moment_Ratio <- safe_calculate(moments::moment(samples, 4) / (moments::moment(samples, 2)^2))
# Create feature data frame
features <- data.frame(
Median = median_val,
Variance = var_val,
IQR = iqr_val,
MAD = mad_val,
Range = range_val,
CV = cv_val,
Root_Mean_Square = rms_val,
Skewness = skewness,
Kurtosis = kurtosis,
Jarque_Bera = jb_stat,
Anderson_Darling = ad_stat,
Shapiro_Wilk = sw_stat,
Shapiro_Francia = sf_stat,
Lilliefors = lf_stat,
Zero_Cross_Rate = zcr,
Gini_Coefficient = gini,
Outliers = outliers,
Peak_to_Trough = pt_ratio,
Spectral_Entropy = spec_entropy,
Energy = energy,
Tail_Weight_Ratio = Tail_Weight_Ratio,
Moment_Ratio = Moment_Ratio,
Zp = zp_stat,
Vasicek_Kmn = vasicek_stat,
stringsAsFactors = FALSE
)
return(features)
}
# ------------------------------------------
# Standardization & Normalization Function
# ------------------------------------------
preprocess_data <- function(train_data) {
# choose only numeric columns(exclude label column)
numeric_train <- train_data[, sapply(train_data, is.numeric)]
# Step 1: Standardization (Z-score scaling: mean = 0, std = 1)
preProcStandard <- preProcess(numeric_train, method = c("center", "scale"))
train_std <- predict(preProcStandard, numeric_train)
# Step 2: Normalization (Rescale to [0,1])
preProcNorm <- preProcess(train_std, method = "range")
train_norm <- predict(preProcNorm, train_std)
# add label comuln back
train_norm$Label <- train_data$Label
return(list(train = train_norm,
preProcStandard = preProcStandard,
preProcNorm = preProcNorm))
}
# --------------------------------------------
# Function to Generation and Extract Features
# --------------------------------------------
generate_data <- function(sample_size, N, dist = "normal", label) {
# row-by-row generation
data <- do.call(rbind, lapply(1:N, function(x) {
samples <- gen_data(sample_size, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
