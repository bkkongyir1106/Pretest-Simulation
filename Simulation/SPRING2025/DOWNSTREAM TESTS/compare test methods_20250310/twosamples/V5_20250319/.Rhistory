#     })
#
#     results[j] <- mean(alt_stats < crit[1] | alt_stats > crit[2])
#   }
#
#   return(results)
# }
}
# -------------------------------------------------------------------------
# Example usage:
# ------------------------------------------------------------------------
{
# sample_sizes <- c(10, 20, 30, 40, 50)  # Your sample sizes
# set.seed(12345)
# power_results <- bootstrap_two_sample(rnorm(50), rnorm(50),  effect_size = 0.5, alpha = 0.05, n_bootstrap = 1e5, sample_size = sample_sizes)
# # Create a data frame for plotting
# plot_data <- data.frame(
#   SampleSize = sample_sizes,
#   Power = power_results
# )
#
# # Generate the ggplot
# ggplot(plot_data, aes(x = SampleSize, y = Power)) +
#   geom_line(color = "#4E79A7", linewidth = 1) +
#   geom_point(color = "#E15759", size = 3) +
#   scale_y_continuous(limits = c(0, 1), labels = scales::percent) +
#   labs(x = "Sample Size",
#        y = "Statistical Power",
#        title = "Power Analysis by Sample Size",
#        subtitle = "Bootstrap estimation of statistical power") +
#   theme_minimal() +
#   theme(
#     plot.title = element_text(face = "bold", hjust = 0.5),
#     plot.subtitle = element_text(hjust = 0.5),
#     panel.grid.minor = element_blank()
#   )
}
# --------------------Addressing Selection Effects ------------------
# -------------------- Data Splitting -------------------------------
# -------------------------------------------------------------------
{
# set.seed(123)
# dist_sum <- c("Normal", "t", "Exponential", "Chi-Square", "LogNormal")
# N     <- 1e3
# alpha <- 0.05
# n     <- 20
# iteration  <- c()
# type_I_error <- numeric(length(dist_sum))
#
# for(j in seq_along(dist_sum)){
#   dist <- dist_sum[j]
#   iter  <- 0
#   pvals <- numeric(0)
#   while (length(pvals) < N) {
#     iter <- iter + 1
#     x <- generate_data(n, dist)
#
#     if (shapiro.test(x[1:floor(length(x) / 2)])$p.value > alpha) {
#       pvals <- c(pvals, t.test(x[(floor(length(x) / 2) + 1):length(x)])$p.value)
#     }
#   }
#   type_I_error[j] <- mean(pvals < alpha)
#   iteration[j] <- iter
# }
# # Print Results
# cat("Type I error rate:", type_I_error, "\n")
# cat("Total iterations performed:", iteration, "\n")
}
# Load user-defined functions
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/DOWNSTREAM TESTS/Comparing Type I error for different test methods")
source("~/Desktop/OSU/Research/Pretest-Simulation/functions/User_defined_functions.R")
# Set seed and simulation parameters
set.seed(12345)
Nsim <- 1e3
sample_size <- c(8, 10, 15, 20, 25, 30, 50)
distribution <- c("Normal", "Exponential", "LogNormal")
alpha <- 0.05
n_boot <- 1e3
effect_size = 0.0
# Preallocate matrices to store the type I error rates.
# Rows correspond to sample sizes and columns to distributions.
error.t.test_mat           <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.mw_u.test_mat        <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.t_mw_u.test_mat      <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.split.test_mat       <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.permutation.test_mat <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.bootstrap.test_mat   <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.t.test_perm.test_mat <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
# Parallel setup - use makePSOCKcluster() for both windows and mac
ncores <- parallel::detectCores() - 1
cl <- parallel::makePSOCKcluster(ncores)
registerDoSNOW(cl)
# Set up a progress bar
total_iter <- length(distribution) * length(sample_size)
current_iter <- 0
pb <- txtProgressBar(min = 0, max = total_iter, style = 3)
# Loop over distributions and sample sizes
for (d in seq_along(distribution)) {
dist_current <- distribution[d]
for (i in seq_along(sample_size)) {
n <- sample_size[i]
# Run simulation iterations in parallel for current combination
sim_results <- foreach(j = 1:Nsim, .combine = rbind,
.export = c("generate_data", "TwoSample.test", "bootstrap_two_sample_test")) %dopar% {
# Generate data for current sample size and distribution
x <- generate_data(n, dist_current)
y <- generate_data(n, dist_current)
# Perform the various tests and record 1 if p-value < alpha (or the bootstrap returns rejection indicator)
c(
t_test    = as.numeric(TwoSample.test(x, y, test = "t", alpha = alpha, effect_size = effect_size) < alpha),
wilcox    = as.numeric(TwoSample.test(x, y, test = "Wilcox", alpha = alpha, effect_size = effect_size) < alpha),
t_wilcox  = as.numeric(TwoSample.test(x, y, test = "t_Wilcox", alpha = alpha, effect_size = effect_size) < alpha),
split     = as.numeric(TwoSample.test(x, y, test = "2stage.split", alpha = alpha, effect_size = effect_size) < alpha),
perm      = as.numeric(TwoSample.test(x, y, test = "perm", alpha = alpha, effect_size = effect_size, B = n_boot) < alpha),
boot      = bootstrap_two_sample_test(x, y, effect_size = effect_size, alpha, n_bootstrap = n_boot),
t.test_perm.test = if(shapiro.test(x)$p.value > alpha & shapiro.test(y)$p.value > alpha){
as.numeric(TwoSample.test(x, y, test = "t", alpha = alpha, effect_size = effect_size) < alpha)
}else{
as.numeric(TwoSample.test(x, y, test = "perm", alpha = alpha, effect_size = effect_size, B = n_boot) < alpha)
}
)
}
# Calculate type I error rates as the proportion of rejections for each test
error.t.test_mat[i, d]           <- mean(sim_results[, "t_test"])
error.mw_u.test_mat[i, d]        <- mean(sim_results[, "wilcox"])
error.t_mw_u.test_mat[i, d]      <- mean(sim_results[, "t_wilcox"])
error.split.test_mat[i, d]       <- mean(sim_results[, "split"])
error.permutation.test_mat[i, d] <- mean(sim_results[, "perm"])
error.bootstrap.test_mat[i, d]   <- mean(sim_results[, "boot"])
error.t.test_perm.test_mat[i, d]   <- mean(sim_results[, "t.test_perm.test"])
# Update progress bar and print a message for the current combination
current_iter <- current_iter + 1
setTxtProgressBar(pb, current_iter)
message(sprintf("Completed: sample size %d, distribution %s", n, dist_current))
}
}
# Load user-defined functions
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/DOWNSTREAM TESTS/Comparing Type I error for different test methods")
source("~/Desktop/OSU/Research/Pretest-Simulation/functions/User_defined_functions.R")
# Set seed and simulation parameters
set.seed(12345)
Nsim <- 1e2
sample_size <- c(8, 10, 15, 20, 25, 30, 50)
distribution <- c("Normal", "Exponential", "LogNormal")
alpha <- 0.05
n_boot <- 1e2
effect_size = 0.0
# Preallocate matrices to store the type I error rates.
# Rows correspond to sample sizes and columns to distributions.
error.t.test_mat           <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.mw_u.test_mat        <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.t_mw_u.test_mat      <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.split.test_mat       <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.permutation.test_mat <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.bootstrap.test_mat   <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
error.t.test_perm.test_mat <- matrix(NA, nrow = length(sample_size), ncol = length(distribution))
# Parallel setup - use makePSOCKcluster() for both windows and mac
ncores <- parallel::detectCores() - 1
cl <- parallel::makePSOCKcluster(ncores)
registerDoSNOW(cl)
# Set up a progress bar
total_iter <- length(distribution) * length(sample_size)
current_iter <- 0
pb <- txtProgressBar(min = 0, max = total_iter, style = 3)
# Loop over distributions and sample sizes
for (d in seq_along(distribution)) {
dist_current <- distribution[d]
for (i in seq_along(sample_size)) {
n <- sample_size[i]
# Run simulation iterations in parallel for current combination
sim_results <- foreach(j = 1:Nsim, .combine = rbind,
.export = c("generate_data", "TwoSample.test", "bootstrap_two_sample_test")) %dopar% {
# Generate data for current sample size and distribution
x <- generate_data(n, dist_current)
y <- generate_data(n, dist_current)
# Perform the various tests and record 1 if p-value < alpha (or the bootstrap returns rejection indicator)
c(
t_test    = as.numeric(TwoSample.test(x, y, test = "t", alpha = alpha, effect_size = effect_size) < alpha),
wilcox    = as.numeric(TwoSample.test(x, y, test = "Wilcox", alpha = alpha, effect_size = effect_size) < alpha),
t_wilcox  = as.numeric(TwoSample.test(x, y, test = "t_Wilcox", alpha = alpha, effect_size = effect_size) < alpha),
split     = as.numeric(TwoSample.test(x, y, test = "2stage.split", alpha = alpha, effect_size = effect_size) < alpha),
perm      = as.numeric(TwoSample.test(x, y, test = "perm", alpha = alpha, effect_size = effect_size, B = n_boot) < alpha),
boot      = bootstrap_two_sample_test(x, y, effect_size = effect_size, alpha, n_bootstrap = n_boot, sample_size = n),
t.test_perm.test = if(shapiro.test(x)$p.value > alpha & shapiro.test(y)$p.value > alpha){
as.numeric(TwoSample.test(x, y, test = "t", alpha = alpha, effect_size = effect_size) < alpha)
}else{
as.numeric(TwoSample.test(x, y, test = "perm", alpha = alpha, effect_size = effect_size, B = n_boot) < alpha)
}
)
}
# Calculate type I error rates as the proportion of rejections for each test
error.t.test_mat[i, d]           <- mean(sim_results[, "t_test"])
error.mw_u.test_mat[i, d]        <- mean(sim_results[, "wilcox"])
error.t_mw_u.test_mat[i, d]      <- mean(sim_results[, "t_wilcox"])
error.split.test_mat[i, d]       <- mean(sim_results[, "split"])
error.permutation.test_mat[i, d] <- mean(sim_results[, "perm"])
error.bootstrap.test_mat[i, d]   <- mean(sim_results[, "boot"])
error.t.test_perm.test_mat[i, d]   <- mean(sim_results[, "t.test_perm.test"])
# Update progress bar and print a message for the current combination
current_iter <- current_iter + 1
setTxtProgressBar(pb, current_iter)
message(sprintf("Completed: sample size %d, distribution %s", n, dist_current))
}
}
# Clean up the progress bar and stop the cluster
close(pb)
stopCluster(cl)
# Name the rows and columns of each result matrix
rownames(error.t.test_mat)           <- sample_size
colnames(error.t.test_mat)            <- distribution
rownames(error.mw_u.test_mat)         <- sample_size
colnames(error.mw_u.test_mat)          <- distribution
rownames(error.t_mw_u.test_mat)       <- sample_size
colnames(error.t_mw_u.test_mat)        <- distribution
rownames(error.split.test_mat)        <- sample_size
colnames(error.split.test_mat)         <- distribution
rownames(error.permutation.test_mat)  <- sample_size
colnames(error.permutation.test_mat)   <- distribution
rownames(error.bootstrap.test_mat)    <- sample_size
colnames(error.bootstrap.test_mat)     <- distribution
rownames(error.t.test_perm.test_mat)    <- sample_size
colnames(error.t.test_perm.test_mat)     <- distribution
# Save the results
save(error.t.test_mat, error.mw_u.test_mat, error.t_mw_u.test_mat,
error.split.test_mat, error.permutation.test_mat, error.bootstrap.test_mat, error.t.test_perm.test_mat,
file = "two_sample.compare_type_I_error_results.RData")
error.t.test_mat
error.mw_u.test_mat
error.t.test_perm.test_mat
error.bootstrap.test_mat
close(pb)
rm(list = ls())
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest, gbm, lawstat, infotheo, ineq, caret, pROC, ROCR,
randomForest, evd, discretization, nnet, ggplot2)
# ---------------------------
# Set Directories & Load Functions
# ---------------------------
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/ML Models/fun.R")
# ---------------------------
# Define Feature Extraction Functions
# ---------------------------
calculate_zero_crossing_rate <- function(samples) {
signs <- samples > 0
zero_crossings <- sum(abs(diff(signs)))
return(zero_crossings / (length(samples) - 1))
}
calculate_gini_coefficient <- function(samples) {
samples_abs <- abs(samples - min(samples))
return(ineq(samples_abs, type = "Gini"))
}
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
return(sum(samples < (qnt[1] - H) | samples > (qnt[2] + H)))
}
calculate_shapiro_wilk <- function(samples) {
return(as.numeric(shapiro.test(samples)$statistic))
}
calculate_shapiro_francia <- function(samples) {
return(as.numeric(lawstat::sf.test(samples)$statistic))
}
calculate_lilliefors <- function(samples) {
return(as.numeric(nortest::lillie.test(samples)$statistic))
}
calculate_cramer_von_mises <- function(samples) {
return(as.numeric(nortest::cvm.test(samples)$statistic))
}
calculate_entropy <- function(samples) {
return(infotheo::entropy(discretize(samples, nbins = 10)))
}
calculate_mad <- function(samples) {
return(mean(abs(samples - mean(samples))))
}
calculate_peak_to_trough <- function(samples) {
return(max(samples) / abs(min(samples)))
}
calculate_sample_size <- function(samples) {
return(length(samples))
}
calculate_range <- function(samples) {
return(max(samples) - min(samples))
}
calculate_cv <- function(samples) {
return(sd(samples) / mean(samples))
}
calculate_energy <- function(samples) {
return(sum(samples^2))
}
calculate_features <- function(samples) {
skewness <- e1071::skewness(samples)
kurtosis <- e1071::kurtosis(samples)
jb_stat <- as.numeric(tseries::jarque.bera.test(samples)$statistic)
ad_stat <- as.numeric(nortest::ad.test(samples)$statistic)
zero_crossing_rate <- calculate_zero_crossing_rate(samples)
gini_coefficient <- calculate_gini_coefficient(samples)
outliers <- calculate_outliers(samples)
shapiro_wilk <- calculate_shapiro_wilk(samples)
lilliefors_stat <- calculate_lilliefors(samples)
cramer_von_mises <- calculate_cramer_von_mises(samples)
range_val <- calculate_range(samples)
cv <- calculate_cv(samples)
energy <- calculate_energy(samples)
features <- data.frame(
#JB_Statistics incorporate both skewness and kurtosis, so we can exclude them
Skewness = skewness,
Kurtosis = kurtosis,
JB_Statistic = jb_stat,
AD_Statistic = ad_stat,
Zero_Crossing_Rate = zero_crossing_rate,
Outliers = outliers,
Shapiro_Wilk = shapiro_wilk,
Liliefors = lilliefors_stat,
Cramer_Von_Mises = cramer_von_mises,
Range = range_val,
Coefficient_of_Variation = cv,
Energy = energy
)
return(features)
}
# ---------------------------
# Standardization & Normalization Function
# ---------------------------
preprocess_data <- function(train_data, test_data) {
numeric_train <- train_data[, sapply(train_data, is.numeric)]
# Step 1: Standardization (Z-score scaling: mean = 0, std = 1)
preProcStandard <- preProcess(numeric_train, method = c("center", "scale"))
train_std <- predict(preProcStandard, numeric_train)
test_std  <- predict(preProcStandard, test_data[, sapply(test_data, is.numeric)])
# Step 2: Normalization (Rescale to [0,1])
preProcNorm <- preProcess(train_std, method = "range")
train_norm <- predict(preProcNorm, train_std)
test_norm  <- predict(preProcNorm, test_std)
train_norm$Label <- train_data$Label
test_norm$Label  <- test_data$Label
return(list(train = train_norm, test = test_norm, preProcStandard = preProcStandard, preProcNorm = preProcNorm))
}
# ---------------------------
# Data Generation Function (balanced sample size across groups)
# ---------------------------
generate_data <- function(sample_size, N, dist = "normal", label) {
data <- do.call(rbind, lapply(1:N, function(x) {
samples <- generate_samples(sample_size, dist)  # generate_samples() must be defined in fun.R
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
# ---------------------------
# Set Seed for Reproducibility and Generate Training Data
# ---------------------------
set.seed(12345)
sample_size <- 8  # consistent sample size per dataset
N <- 500            # number of samples per distribution
normal_data1 <- generate_data(sample_size, 5*N, "normal", "Normal")
normal_data2 <- generate_data(sample_size, 5*N, "normal", "Normal")
# Generate non-normal data from several distributions
lognormal <- generate_data(sample_size, N, "LogNormal", "Non_Normal")
chisq_data   <- generate_data(sample_size, N, "Chi-Square", "Non_Normal")
exp_data     <- generate_data(sample_size, N, "Exponential", "Non_Normal")
Weibull      <- generate_data(sample_size, N, "Weibull", "Non_Normal")
Pareto      <- generate_data(sample_size, N, "Pareto", "Non_Normal")
Laplace      <- generate_data(sample_size, N, "Laplace", "Non_Normal")
Gamma        <- generate_data(sample_size, N, "Gamma", "Non_Normal")
#Gumbel       <- generate_data(sample_size, N, "Gumbel", "Non_Normal")
Uniform      <- generate_data(sample_size, N, "Uniform", "Non_Normal")
t_data       <- generate_data(sample_size, N, "t", "Non_Normal")
t10_data       <- generate_data(sample_size, N, "t", "Non_Normal")
non_normal_data <- rbind(lognormal, chisq_data, exp_data, Weibull, Pareto, Laplace, Gamma, Uniform, t_data, t10_data)
data_all <- rbind(normal_data1, normal_data2, non_normal_data)
data_all$Label <- as.factor(data_all$Label)
data_all <- rbind(normal_data, non_normal_data)
non_normal_data <- rbind(lognormal, chisq_data, exp_data, Weibull, Pareto, Laplace, Gamma, Uniform, t_data, t10_data)
data_all <- rbind(normal_data1, normal_data2, non_normal_data)
data_all$Label <- as.factor(data_all$Label)
# ---------------------------
# Split Data (70% Train, 30% Test)
# ---------------------------
set.seed(123)
trainIndex <- createDataPartition(data_all$Label, p = 0.7, list = FALSE)
train_data <- data_all[trainIndex, ]
test_data  <- data_all[-trainIndex, ]
# ---------------------------
# Standardize & Normalize Data
# ---------------------------
norm_result <- preprocess_data(train_data, test_data)
train_norm <- norm_result$train
test_norm  <- norm_result$test
# ---------------------------
# Define Common Training Control for Cross-Validation
# ---------------------------
ctrl <- trainControl(method = "cv", number = 10,
summaryFunction = twoClassSummary,
classProbs = TRUE,
savePredictions = "final")
# ---------------------------
# Train Machine Learning Models
# ---------------------------
log_model <- train(Label ~ .,
data = train_norm,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "ROC")
log_pred <- predict(log_model, newdata = test_norm)
cat("Test Results for: Logistic Regression Model\n")
print(confusionMatrix(log_pred, test_norm$Label))
# Load user-defined functions
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/DOWNSTREAM TESTS/compare test methods_20250310/twosamples/V5_20250319")
source("~/Desktop/OSU/Research/Pretest-Simulation/functions/User_defined_functions.R")
source("~/Desktop/OSU/Research/Pretest-Simulation/functions/utility.R")
# set up cores for parallel processing
par_set <- function(cores_reserve = 2)
{
cores = parallel::detectCores()
cores_use <- cores - cores_reserve
if(Sys.info()["sysname"] == "Windows"){
# make a socket for both Windows & Unix-like
cl <- parallel::makeCluster(cores_use)
doParallel::registerDoParallel(cl)
}else{
# make a socket cluster for Unix-like
cl <- snow::makeSOCKcluster(cores_use)
doSNOW::registerDoSNOW(cl)
}
foreach::getDoParWorkers()
return(cl)
}
# Function to close the cluster
close_cluster <- function(cl) {
parallel::stopCluster(cl)
}
## Set up the simulation parameters
{
Nsim <- 1e4
sample_size <- c(8, 10, 15, 20, 25, 30)
distribution <- c("Normal", "Exponential", "LogNormal")
alpha <- 0.05
B <- 1e3
effect_size <- 0.0
}
# Progress taskbar setup
{
my_cl <- par_set(cores_reserve = 2)
ntasks <- length(sample_size) * length(distribution)
pb <- txtProgressBar(max=ntasks, style=3)
progress <- function(n) setTxtProgressBar(pb, n)
opts <- list(progress=progress)
}
# Track simulation start time
start_time <- Sys.time()
cat("Simulation started at:", format(start_time), "\n\n")
## Perform simulation
#system.time({
sim_out <- foreach(n = sample_size,
.packages = c("LaplacesDemon", "VGAM"), # Put Difficult Packages here
.options.snow=opts) %:%
foreach(dist = distribution) %dopar%
{
set.seed(12345)
pval_sw <- pval_t <- pval_perm <- pval_adaptive_t_perm <- pval_conditional <- numeric(Nsim)
for (i in 1:Nsim) {
x <- generate_data(n, dist)
y <- generate_data(n, dist)
# perform Shapiro-Wilk test
pval_sw[i] <- shapiro.test(x)$p.value
# perform t-test
pval_t[i] <- t.test(x, y, mu = effect_size)$p.value
# perform permutation test
observe_stat <- TwoSample_test_statistic(x, y)
data <- c(x, y)
permuted_stat <- numeric(B)
for (j in 1:B) {
sample_data <- sample(data)
sample_x <- sample_data[1:length(x)]
sample_y <- sample_data[(length(x) + 1):(length(x) + length(y))]
permuted_stat[j] <- TwoSample_test_statistic(sample_x, sample_y + effect_size)
}
pval_perm[i] <- mean(abs(permuted_stat) >= abs(observe_stat))
# Adaptive t/permutation
if(pval_sw[i] > alpha){
pval_adaptive_t_perm[i] <- t.test(x, y, mu = effect_size)$p.value
} else {
observe_stat <- TwoSample_test_statistic(x, y)
data <- c(x, y)
permuted_stat <- numeric(B)
for (j in 1:B) {
sample_data <- sample(data)
sample_x <- sample_data[1:length(x)]
sample_y <- sample_data[(length(x) + 1):(length(x) + length(y))]
permuted_stat[j] <- TwoSample_test_statistic(sample_x, sample_y + effect_size)
}
pval_adaptive_t_perm[i] <- mean(abs(permuted_stat) >= abs(observe_stat))
}
# Conditional t-test error only if both samples are normal
if (pval_sw[i] > alpha && shapiro.test(y)$p.value > alpha) {
pval_conditional[i] <- t.test(x, y, mu = effect_size)$p.value
} else {
pval_conditional[i] <- NA
}
}
# calculate results
results <- list(power_sw = mean(pval_sw < alpha),
error_t = mean(pval_t < alpha),
error_perm = mean(pval_perm < alpha),
error_adaptive_t_perm = mean(pval_adaptive_t_perm < alpha),
error_conditional = round(mean(pval_conditional < alpha, na.rm = TRUE), 3))
return(results)
#return(list(pval_sw = pval_sw,pval_t = pval_t, pval_perm = pval_perm, pval_adaptive_t_perm = pval_adaptive_t_perm,pval_conditional = pval_conditional))
}
