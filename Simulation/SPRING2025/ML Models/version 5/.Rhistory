num.sim <- 100
# Normal data
normal_data1 <- generate_data(sample_size, 2*num.sim, "normal", "Normal")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/ML Models/fun.R")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/ML Models/fun.R")
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest, gbm, lawstat, infotheo, ineq, caret, pROC, ROCR, randomForest, evd, discretization, nnet, ggplot2)
# ---------------------------
# Define Feature Extraction Functions
# ---------------------------
calculate_zero_crossing_rate <- function(samples) {
signs <- samples > 0
zero_crossings <- sum(abs(diff(signs)))
return(zero_crossings / (length(samples) - 1))
}
calculate_gini_coefficient <- function(samples) {
samples_abs <- abs(samples - min(samples))
return(ineq(samples_abs, type = "Gini"))
}
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
return(sum(samples < (qnt[1] - H) | samples > (qnt[2] + H)))
}
calculate_entropy <- function(samples) {
return(infotheo::entropy(discretize(samples, nbins = 10)))
}
calculate_peak_to_trough <- function(samples) max(samples) / abs(min(samples))
calculate_box_test  <- function(samples) {
as.numeric(Box.test(samples, lag = 1, type = "Ljung-Box")$statistic)
}
calculate_sample_entropy <- function(samples) {
return(infotheo::entropy(discretize(samples, nbins = 10)))
}
calculate_spectral_entropy <- function(samples) {
spec <- stats::spec.pgram(samples, plot = FALSE)
p <- spec$spec / sum(spec$spec)
-sum(p * log(p))
}
calculate_spectral_centroid <- function(samples) {
spec <- stats::spec.pgram(samples, plot = FALSE)
p <- spec$spec / sum(spec$spec)
freq <- spec$freq
sum(freq * p) / sum(p)
}
# Fractal & complexity measures
calculate_hjorth <- function(samples) {
activity   <- var(samples)
mobility   <- sqrt(var(diff(samples)) / activity)
complexity <- (sqrt(var(diff(diff(samples))) / var(diff(samples)))) / mobility
c(Activity = activity, Mobility = mobility, Complexity = complexity)
}
calculate_fractal_dimension <- function(samples) {
res <- fractaldim::fd.estimate(samples, methods = "madogram")
as.numeric(res$fd)
}
calculate_energy <- function(samples) {
return(sum(samples^2))
}
# Dispersion & variation
calculate_cv             <- function(samples) sd(samples) / mean(samples)
calculate_range          <- function(samples) max(samples) - min(samples)
calculate_entropy        <- function(samples) {
infotheo::entropy(discretize(samples, nbins = 10))
}
# Central tendency, spread & shape
calculate_mean           <- function(samples) mean(samples)
calculate_median         <- function(samples) median(samples)
calculate_variance       <- function(samples) var(samples)
calculate_iqr            <- function(samples) IQR(samples)
calculate_mad            <- function(samples) mad(samples)
calculate_rms            <- function(samples) sqrt(mean(samples^2))
# ---------------------------
# extract all features
# ---------------------------
calculate_features <- function(samples) {
# Central & spread
mean_val        <- mean(samples)
median_val      <- median(samples)
var_val         <- var(samples)
iqr_val         <- IQR(samples)
mad_val         <- mad(samples)
range_val       <- max(samples) - min(samples)
cv_val          <- calculate_cv(samples)
rms_val         <- calculate_rms(samples)
# Normality & shape - convert to categorical
skewness_val    <- e1071::skewness(samples)
kurtosis_val    <- e1071::kurtosis(samples)
skewness        <- ifelse(skewness_val >= -2 & skewness_val <= 2, "yes", "no")
kurtosis        <- ifelse(kurtosis_val >= -0.5 & kurtosis_val <= 0.5, "yes", "no")
# Normality tests - using p-values
jb_pval         <- tseries::jarque.bera.test(samples)$p.value
ad_pval         <- nortest::ad.test(samples)$p.value
sw_pval         <- shapiro.test(samples)$p.value
sf_pval         <- nortest::sf.test(samples)$p.value
lf_pval         <- nortest::lillie.test(samples)$p.value
cvm_pval        <- nortest::cvm.test(samples)$p.value
# NEW FEATURE: Majority Normality based on p-values
pvals <- c(jb_pval, ad_pval, sw_pval, sf_pval, lf_pval, cvm_pval)
rejections <- sum(pvals < 0.05)
majority_normality <- ifelse(rejections >= 3, "no", "yes")
# Time & distributional
zcr             <- calculate_zero_crossing_rate(samples)
gini            <- calculate_gini_coefficient(samples)
outliers        <- calculate_outliers(samples)
# Entropy & size
entropy_val     <- calculate_entropy(samples)
samp_entropy    <- calculate_sample_entropy(samples)
# Time-domain signal features
pt_ratio        <- calculate_peak_to_trough(samples)
box_val         <- calculate_box_test(samples)
# Frequency-domain
spec_entropy    <- calculate_spectral_entropy(samples)
spec_centroid   <- calculate_spectral_centroid(samples)
# Fractal & complexity
fd_val          <- calculate_fractal_dimension(samples)
hjorth_vals     <- calculate_hjorth(samples)
energy          <- calculate_energy(samples)
# Create dataframe - EXCLUDED INDIVIDUAL NORMALITY TESTS
features <- data.frame(
Skewness                = skewness,
Kurtosis                = kurtosis,
Majority_Normality      = majority_normality,
Zero_Cross_Rate         = zcr,
Gini_Coefficient        = gini,
Outliers                = outliers,
Mean                    = mean_val,
Median                  = median_val,
Variance                = var_val,
IQR                     = iqr_val,
MAD                     = mad_val,
Range                   = range_val,
CV                      = cv_val,
Root_Mean_Square        = rms_val,
energy                  = energy,
Peak_to_Trough          = pt_ratio,
Spectral_Entropy        = spec_entropy,
Spectral_Centroid       = spec_centroid,
Box_Ljung_Stat          = box_val,
Fractal_Dimension       = fd_val,
Hjorth_Activity         = hjorth_vals["Activity"],
Hjorth_Mobility         = hjorth_vals["Mobility"],
Hjorth_Complexity       = hjorth_vals["Complexity"]
)
return(features)
}
# ---------------------------
# Standardization & Normalization Function
# ---------------------------
preprocess_data <- function(train_data) {
# Identify numeric columns (excluding the Label)
numeric_cols <- sapply(train_data, is.numeric)
# Separate numeric and non-numeric data
numeric_train <- train_data[, numeric_cols, drop = FALSE]
non_numeric_train <- train_data[, !numeric_cols, drop = FALSE]
# Step 1: Standardization (Z-score scaling: mean = 0, std = 1)
preProcStandard <- preProcess(numeric_train, method = c("center", "scale"))
train_std <- predict(preProcStandard, numeric_train)
# Step 2: Normalization (Rescale to [0,1])
preProcNorm <- preProcess(train_std, method = "range")
train_norm <- predict(preProcNorm, train_std)
# Combine processed numeric features with non-numeric features
train_processed <- cbind(train_norm, non_numeric_train)
return(list(train = train_processed,
preProcStandard = preProcStandard,
preProcNorm = preProcNorm))
}
# ---------------------------
# Data Generation Function
# ---------------------------
generate_data <- function(sample_size, N, dist = "normal", label) {
data <- do.call(rbind, lapply(1:N, function(x) {
samples <- generate_samples(sample_size, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
set.seed(12345)
sample_size <- 10
num.sim <- 100
# Normal data
normal_data1 <- generate_data(sample_size, 2*num.sim, "normal", "Normal")
normal_data2 <- generate_data(sample_size, 2*num.sim, "normal_5", "Normal")
normal_data3 <- generate_data(sample_size, 2*num.sim, "normal_15", "Normal")
normal_data4 <- generate_data(sample_size, 2*num.sim, "normal_25", "Normal")
normal_data5 <- generate_data(sample_size, 2*num.sim, "normal_50", "Normal")
normal_data6 <- generate_data(sample_size, 2*num.sim, "normal_100", "Normal")
# non-normal data
lognormal <- generate_data(sample_size, num.sim, "LogNormal", "Non_Normal")
chisq_data   <- generate_data(sample_size, num.sim, "Chi_Square", "Non_Normal")
exp_data     <- generate_data(sample_size, num.sim, "Exponential", "Non_Normal")
Weibull      <- generate_data(sample_size, num.sim, "Weibull", "Non_Normal")
Pareto      <- generate_data(sample_size, num.sim, "Pareto", "Non_Normal")
Laplace      <- generate_data(sample_size, num.sim, "Laplace", "Non_Normal")
Gamma        <- generate_data(sample_size, num.sim, "Gamma", "Non_Normal")
Uniform      <- generate_data(sample_size, num.sim, "Uniform", "Non_Normal")
t       <- generate_data(sample_size, num.sim, "t", "Non_Normal")
t_15       <- generate_data(sample_size, num.sim, "t_5", "Non_Normal")
t_25       <- generate_data(sample_size, num.sim, "t_15", "Non_Normal")
beta       <- generate_data(sample_size, num.sim, "beta", "Non_Normal")
non_normal_data <- rbind(lognormal, chisq_data,  exp_data, Weibull, Pareto, Laplace, Gamma, Uniform, t, t_15)
normal_data <- rbind(normal_data1, normal_data2, normal_data3, normal_data5, normal_data6)
data_all <- rbind(normal_data, non_normal_data)
data_all$Label <- as.factor(data_all$Label)
# ---------------------------
# Standardize & Normalize Data
# ---------------------------
norm_result <- preprocess_data(data_all)
train_norm <- norm_result$train
# define appropriate class reference
train_norm$Label <- relevel(train_norm$Label, ref = "Non_Normal")
# ---------------------------
# Define Common Training Control for Cross-Validation
set.seed(12345)
# ---------------------------
ctrl <- trainControl(method = "cv", number = 10,
summaryFunction = defaultSummary,
classProbs = TRUE,
search = "grid",
savePredictions = "final")
# ---------------------------
# Train Machine Learning Models
# ---------------------------
# Logistic Regression
log_model <- train(Label ~ .,
data = train_norm,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "Accuracy")
# Random Forest
rf_model <- train(Label ~ .,
data = train_norm,
method = "rf",
trControl = ctrl,
metric = "Accuracy")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/SPRING2025/ML Models/fun.R")
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest, gbm, lawstat, infotheo, ineq, caret, pROC, ROCR, randomForest, evd, discretization, nnet, ggplot2)
# ---------------------------
# Define Feature Extraction Functions (Safe Version with NA return)
# ---------------------------
calculate_zero_crossing_rate <- function(samples) {
if (length(samples) < 2) return(NA)
signs <- samples > 0
zero_crossings <- sum(abs(diff(signs)))
zero_crossings / (length(samples) - 1)
}
calculate_gini_coefficient <- function(samples) {
samples_abs <- abs(samples - min(samples))
if (all(samples_abs == 0)) return(NA)
ineq(samples_abs, type = "Gini")
}
calculate_outliers <- function(samples) {
if (length(samples) < 3) return(NA)
qnt <- quantile(samples, probs = c(0.25, 0.75), na.rm = TRUE)
H <- 1.5 * IQR(samples, na.rm = TRUE)
sum(samples < (qnt[1] - H) | samples > (qnt[2] + H))
}
calculate_entropy <- function(samples) {
if (length(unique(samples)) < 2) return(NA)
infotheo::entropy(discretize(samples, nbins = 10))
}
calculate_peak_to_trough <- function(samples) {
min_val <- min(samples)
if (abs(min_val) < .Machine$double.eps) return(NA)
max_val <- max(samples)
max_val / abs(min_val)
}
calculate_box_test <- function(samples) {
if (var(samples) < .Machine$double.eps) return(NA)
tryCatch({
as.numeric(Box.test(samples, lag = 1, type = "Ljung-Box")$statistic)
}, error = function(e) NA)
}
calculate_cv <- function(samples) {
m <- mean(samples)
if (abs(m) < .Machine$double.eps) return(NA)
sd(samples) / m
}
calculate_spectral_entropy <- function(samples) {
if (var(samples) < .Machine$double.eps) return(NA)
spec <- stats::spec.pgram(samples, plot = FALSE)
p <- spec$spec / sum(spec$spec)
-sum(p * log(p + .Machine$double.eps))
}
calculate_spectral_centroid <- function(samples) {
if (var(samples) < .Machine$double.eps) return(NA)
spec <- stats::spec.pgram(samples, plot = FALSE)
p <- spec$spec / sum(spec$spec)
freq <- spec$freq
sum(freq * p) / sum(p)
}
calculate_hjorth <- function(samples) {
activity <- var(samples)
if (activity < .Machine$double.eps) {
return(c(Activity = NA, Mobility = NA, Complexity = NA))
}
first_diff <- diff(samples)
var_first_diff <- var(first_diff)
mobility <- sqrt(var_first_diff / activity)
if (var_first_diff < .Machine$double.eps) {
complexity <- NA
} else {
second_diff <- diff(first_diff)
var_second_diff <- var(second_diff)
complexity <- (sqrt(var_second_diff / var_first_diff)) / mobility
}
c(Activity = activity, Mobility = mobility, Complexity = complexity)
}
calculate_fractal_dimension <- function(samples) {
if (var(samples) < .Machine$double.eps) return(NA)
tryCatch({
res <- fractaldim::fd.estimate(samples, methods = "madogram")
as.numeric(res$fd)
}, error = function(e) NA)
}
safe_norm_test <- function(fun, samples) {
if (var(samples) < .Machine$double.eps) return(NA)
tryCatch({
as.numeric(fun(samples)$statistic)
}, error = function(e) NA)
}
# ---------------------------
# Improved Feature Extraction
# ---------------------------
calculate_features <- function(samples) {
# Central & spread
mean_val        <- mean(samples)
median_val      <- median(samples)
var_val         <- var(samples)
iqr_val         <- IQR(samples, na.rm = TRUE)
mad_val         <- mad(samples, na.rm = TRUE)
range_val       <- max(samples) - min(samples)
cv_val          <- calculate_cv(samples)
rms_val         <- sqrt(mean(samples^2))
# Normality & shape
skewness        <- e1071::skewness(samples)
kurtosis        <- e1071::kurtosis(samples)
jb_stat         <- safe_norm_test(tseries::jarque.bera.test, samples)
ad_stat         <- safe_norm_test(nortest::ad.test, samples)
sw_stat         <- safe_norm_test(shapiro.test, samples)
sf_stat         <- safe_norm_test(nortest::sf.test, samples)
lf_stat         <- safe_norm_test(nortest::lillie.test, samples)
cvm_stat        <- safe_norm_test(nortest::cvm.test, samples)
# Time & distributional
zcr             <- calculate_zero_crossing_rate(samples)
gini            <- calculate_gini_coefficient(samples)
outliers        <- calculate_outliers(samples)
# Entropy & size
entropy_val     <- calculate_entropy(samples)
samp_entropy    <- entropy_val
# Time-domain signal features
pt_ratio        <- calculate_peak_to_trough(samples)
box_val         <- calculate_box_test(samples)
# Frequency-domain
spec_entropy    <- calculate_spectral_entropy(samples)
spec_centroid   <- calculate_spectral_centroid(samples)
# Fractal & complexity
fd_val          <- calculate_fractal_dimension(samples)
hjorth_vals     <- calculate_hjorth(samples)
energy          <- sum(samples^2)
# Create dataframe
features <- data.frame(
Skewness                = skewness,
Kurtosis                = kurtosis,
Jarque_Bera             = jb_stat,
Anderson_Darling        = ad_stat,
Shapiro_Wilk            = sw_stat,
Shapiro_Francia         = sf_stat,
Lilliefors              = lf_stat,
Cramer_Von_Misse        = cvm_stat,
Zero_Cross_Rate         = zcr,
Gini_Coefficient        = gini,
Outliers                = outliers,
Mean                    = mean_val,
Median                  = median_val,
Variance                = var_val,
IQR                     = iqr_val,
MAD                     = mad_val,
Range                   = range_val,
CV                      = cv_val,
Root_Mean_Square        = rms_val,
energy                  = energy,
Peak_to_Trough          = pt_ratio,
Spectral_Entropy        = spec_entropy,
Spectral_Centroid       = spec_centroid,
Box_Ljung_Stat          = box_val,
Fractal_Dimension       = fd_val,
Hjorth_Activity         = hjorth_vals["Activity"],
Hjorth_Mobility         = hjorth_vals["Mobility"],
Hjorth_Complexity       = hjorth_vals["Complexity"]
)
return(features)
}
# ---------------------------
# Advanced Imputation Function
# ---------------------------
impute_missing <- function(data) {
# Separate features and labels
labels <- data$Label
features <- data[, !colnames(data) %in% "Label"]
# Convert Hjorth features to separate columns
hjorth_cols <- c("Hjorth_Activity", "Hjorth_Mobility", "Hjorth_Complexity")
features[hjorth_cols] <- lapply(features[hjorth_cols], as.numeric)
# First pass: Median imputation for columns with <10% missing
missing_pct <- colMeans(is.na(features))
low_missing_cols <- names(missing_pct[missing_pct < 0.1 & missing_pct > 0])
for (col in low_missing_cols) {
features[is.na(features[[col]]), col] <- median(features[[col]], na.rm = TRUE)
}
# Second pass: kNN imputation for remaining missing values
if (any(is.na(features))) {
pacman::p_load(VIM)
features_imputed <- kNN(features, k = 5, imp_var = FALSE)
} else {
features_imputed <- features
}
# Return complete dataset
features_imputed$Label <- labels
return(features_imputed)
}
# ---------------------------
# Enhanced Preprocessing
# ---------------------------
preprocess_data <- function(train_data) {
# 1. Impute missing values
train_imputed <- impute_missing(train_data)
# 2. Standardization and Normalization
numeric_train <- train_imputed[, sapply(train_imputed, is.numeric)]
preProcStandard <- preProcess(numeric_train, method = c("center", "scale"))
train_std <- predict(preProcStandard, numeric_train)
preProcNorm <- preProcess(train_std, method = "range")
train_norm <- predict(preProcNorm, train_std)
train_norm$Label <- train_imputed$Label
return(list(train = train_norm,
preProcStandard = preProcStandard,
preProcNorm = preProcNorm))
}
# ---------------------------
# Update Data Generation
# ---------------------------
generate_data <- function(sample_size, N, dist = "normal", label) {
data <- do.call(rbind, lapply(1:N, function(x) {
samples <- generate_samples(sample_size, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
set.seed(12345)
sample_size <- 8
num.sim <- 100
# Normal data
normal_data1 <- generate_data(sample_size, 2*num.sim, "normal", "Normal")
normal_data2 <- generate_data(sample_size, 2*num.sim, "normal_5", "Normal")
normal_data3 <- generate_data(sample_size, 2*num.sim, "normal_15", "Normal")
normal_data4 <- generate_data(sample_size, 2*num.sim, "normal_25", "Normal")
normal_data5 <- generate_data(sample_size, 2*num.sim, "normal_50", "Normal")
normal_data6 <- generate_data(sample_size, 2*num.sim, "normal_100", "Normal")
# non-normal data
lognormal <- generate_data(sample_size, num.sim, "LogNormal", "Non_Normal")
chisq_data   <- generate_data(sample_size, num.sim, "Chi_Square", "Non_Normal")
exp_data     <- generate_data(sample_size, num.sim, "Exponential", "Non_Normal")
Weibull      <- generate_data(sample_size, num.sim, "Weibull", "Non_Normal")
Pareto      <- generate_data(sample_size, num.sim, "Pareto", "Non_Normal")
Laplace      <- generate_data(sample_size, num.sim, "Laplace", "Non_Normal")
Gamma        <- generate_data(sample_size, num.sim, "Gamma", "Non_Normal")
Uniform      <- generate_data(sample_size, num.sim, "Uniform", "Non_Normal")
t       <- generate_data(sample_size, num.sim, "t", "Non_Normal")
t_15       <- generate_data(sample_size, num.sim, "t_5", "Non_Normal")
t_25       <- generate_data(sample_size, num.sim, "t_15", "Non_Normal")
beta       <- generate_data(sample_size, num.sim, "beta", "Non_Normal")
non_normal_data <- rbind(lognormal, chisq_data,  exp_data, Weibull, Pareto, Laplace, Gamma, Uniform, t, t_15)
normal_data <- rbind(normal_data1, normal_data2, normal_data3, normal_data5, normal_data6)
data_all <- rbind(normal_data, non_normal_data)
data_all$Label <- as.factor(data_all$Label)
# ---------------------------
# Standardize & Normalize Data
# ---------------------------
norm_result <- preprocess_data(data_all)
train_norm <- norm_result$train
# define appropriate class reference
train_norm$Label <- relevel(train_norm$Label, ref = "Non_Normal")
# ---------------------------
# Define Common Training Control for Cross-Validation
set.seed(12345)
# ---------------------------
ctrl <- trainControl(method = "cv", number = 10,
summaryFunction = defaultSummary,
classProbs = TRUE,
search = "grid",
savePredictions = "final")
# ---------------------------
# Train Machine Learning Models
# ---------------------------
# Logistic Regression
log_model <- train(Label ~ .,
data = train_norm,
method = "glm",
family = "binomial",
trControl = ctrl,
metric = "Accuracy")
# Random Forest
rf_model <- train(Label ~ .,
data = train_norm,
method = "rf",
trControl = ctrl,
metric = "Accuracy")
