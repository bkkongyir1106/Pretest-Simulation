outliers <- sum(samples < (qnt[1] - H) | samples > (qnt[2] + H))
return(outliers)
}
calculate_shapiro_wilk <- function(samples) {
shapiro_stat <- shapiro.test(samples)$statistic
return(shapiro_stat)
}
# Shapiro-Francia Test
calculate_shapiro_francia <- function(samples) {
result <- tryCatch({
lawstat::sf.test(samples)$statistic
}, error = function(e) {
warning("Shapiro-Francia test failed")
return(NA)
})
return(as.numeric(result))
}
# Liliefors Test (Kolmogorov-Smirnov for estimated parameters)
calculate_lilliefors <- function(samples) {
result <- tryCatch({
nortest::lillie.test(samples)$statistic
}, error = function(e) {
warning("Lilliefors test failed")
return(NA)
})
return(as.numeric(result))
}
# Cramer-von Mises Test
calculate_cramer_von_mises <- function(samples) {
result <- tryCatch({
nortest::cvm.test(samples)$statistic
}, error = function(e) {
warning("Cramer-von Mises test failed")
return(NA)
})
return(as.numeric(result))
}
calculate_features <- function(samples) {
skewness <- e1071::skewness(samples)
kurtosis <- e1071::kurtosis(samples)
jb_stat <- tseries::jarque.bera.test(samples)$statistic
ad_stat <- nortest::ad.test(samples)$statistic
zero_crossing_rate <- calculate_zero_crossing_rate(samples)
gini_coefficient <- calculate_gini_coefficient(samples)
outliers <- calculate_outliers(samples)
shapiro_wilk <- calculate_shapiro_wilk(samples)
shapiro_francia <- calculate_shapiro_francia(samples)
lilliefors_stat <- calculate_lilliefors(samples)
cramer_von_mises <- calculate_cramer_von_mises(samples)
features <- data.frame(
Skewness = skewness,
Kurtosis = kurtosis,
JB_Statistic = as.numeric(jb_stat),
AD_Statistic = as.numeric(ad_stat),
Zero_Crossing_Rate = zero_crossing_rate,
Gini_Coefficient = gini_coefficient,
Outliers = outliers,
Shapiro_Wilk = as.numeric(shapiro_wilk),
# Shapiro_Francia = shapiro_francia,
Liliefors = lilliefors_stat,
Cramer_Von_Mises = cramer_von_mises
)
return(features)
}
# Min-Max normalization function
min_max_normalize <- function(data) {
return((data - mean(data)) / (sd(data)))
}
# Normalize all numeric columns except the Label column
normalize_data <- function(data) {
numeric_features <- data[sapply(data, is.numeric)]  # Extract numeric features
normalized_features <- as.data.frame(lapply(numeric_features, min_max_normalize))
normalized_data <- cbind(normalized_features, Label = data$Label)  # Re-attach the Label column
return(normalized_data)
}
# Step 2: Prepare Data for Machine Learning Models
generate_data <- function(n_samples, n_per_sample, dist = "normal", label) {
data <- do.call(rbind, lapply(1:n_samples, function(x) {
samples <- generate_samples(n_per_sample, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
# Generate data
set.seed(123)
normal <- generate_data(500, 10, "normal", "Normal")
stdnormal <- generate_data(500, 10, "Standard Normal", "Normal")
lognorma <- generate_data(100, 10,  "LogNormal", "Non_Normal")
chisq_data <- generate_data(100, 10, "Chi-Square", "Non_Normal")
exp_data <- generate_data(100, 10, "Exponential", "Non_Normal")
Weibull <- generate_data(100, 10, "Weibull", "Non_Normal")
Pareto <- generate_data(100, 10, "Pareto", "Non_Normal")
normal_data <- rbind(normal, stdnormal)
non_normal_data <- rbind(lognorma,chisq_data, Weibull, exp_data, Pareto)
# Combine and shuffle data, excluding entropy and tail index columns
data <- rbind(normal_data, non_normal_data)
data <- data[sample(nrow(data)), ]
# Step 3: Split Data into Training and Test Sets
set.seed(123)
trainIndex <- createDataPartition(data$Label, p = .8, list = FALSE, times = 1)
train_data <- data[trainIndex, ]
test_data  <- data[-trainIndex, ]
# Ensure that Label is a factor
train_data$Label <- as.factor(train_data$Label)
test_data$Label <- as.factor(test_data$Label)
# Normalize the training and test data
Normalized_train_data <- normalize_data(train_data)
Normalized_test_data <- normalize_data(test_data)
# Step 4: Build Machine Learning Models
# Logistic Regression Model
log_model <- train(Label ~ ., data = train_data, method = "glm", family = "binomial")
# Required Libraries
rm(list = ls())
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest,lawstat, infotheo, ineq, caret, pROC, ROCR, randomForest, evd, discretization, nnet,keras,tensorflow, class, ggplot2)
# Clear working environment
rm(list = ls())
# Set directories in local computer
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH/generate_data.R")
calculate_zero_crossing_rate <- function(samples) {
zero_crossings <- sum(diff(samples > 0))
return(zero_crossings / length(samples))
}
calculate_gini_coefficient <- function(samples) {
gini_value <- ineq(samples, type = "Gini")
return(gini_value)
}
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
outliers <- sum(samples < (qnt[1] - H) | samples > (qnt[2] + H))
return(outliers)
}
calculate_shapiro_wilk <- function(samples) {
shapiro_stat <- shapiro.test(samples)$statistic
return(shapiro_stat)
}
# Shapiro-Francia Test
calculate_shapiro_francia <- function(samples) {
result <- tryCatch({
lawstat::sf.test(samples)$statistic
}, error = function(e) {
warning("Shapiro-Francia test failed")
return(NA)
})
return(as.numeric(result))
}
# Liliefors Test (Kolmogorov-Smirnov for estimated parameters)
calculate_lilliefors <- function(samples) {
result <- tryCatch({
nortest::lillie.test(samples)$statistic
}, error = function(e) {
warning("Lilliefors test failed")
return(NA)
})
return(as.numeric(result))
}
# Cramer-von Mises Test
calculate_cramer_von_mises <- function(samples) {
result <- tryCatch({
nortest::cvm.test(samples)$statistic
}, error = function(e) {
warning("Cramer-von Mises test failed")
return(NA)
})
return(as.numeric(result))
}
calculate_features <- function(samples) {
skewness <- e1071::skewness(samples)
kurtosis <- e1071::kurtosis(samples)
jb_stat <- tseries::jarque.bera.test(samples)$statistic
ad_stat <- nortest::ad.test(samples)$statistic
zero_crossing_rate <- calculate_zero_crossing_rate(samples)
gini_coefficient <- calculate_gini_coefficient(samples)
outliers <- calculate_outliers(samples)
shapiro_wilk <- calculate_shapiro_wilk(samples)
shapiro_francia <- calculate_shapiro_francia(samples)
lilliefors_stat <- calculate_lilliefors(samples)
cramer_von_mises <- calculate_cramer_von_mises(samples)
features <- data.frame(
Skewness = skewness,
Kurtosis = kurtosis,
JB_Statistic = as.numeric(jb_stat),
AD_Statistic = as.numeric(ad_stat),
Zero_Crossing_Rate = zero_crossing_rate,
Gini_Coefficient = gini_coefficient,
Outliers = outliers,
Shapiro_Wilk = as.numeric(shapiro_wilk),
# Shapiro_Francia = shapiro_francia,
Liliefors = lilliefors_stat,
Cramer_Von_Mises = cramer_von_mises
)
return(features)
}
# Min-Max normalization function
min_max_normalize <- function(data) {
return((data - mean(data)) / (sd(data)))
}
# Normalize all numeric columns except the Label column
normalize_data <- function(data) {
numeric_features <- data[sapply(data, is.numeric)]  # Extract numeric features
normalized_features <- as.data.frame(lapply(numeric_features, min_max_normalize))
normalized_data <- cbind(normalized_features, Label = data$Label)  # Re-attach the Label column
return(normalized_data)
}
# Step 2: Prepare Data for Machine Learning Models
generate_data <- function(n_samples, n_per_sample, dist = "normal", label) {
data <- do.call(rbind, lapply(1:n_samples, function(x) {
samples <- generate_samples(n_per_sample, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
# Generate data
set.seed(123)
normal <- generate_data(500, 10, "normal", "Normal")
stdnormal <- generate_data(500, 10, "Standard Normal", "Normal")
lognorma <- generate_data(100, 10,  "LogNormal", "Non_Normal")
chisq_data <- generate_data(100, 10, "Chi-Square", "Non_Normal")
exp_data <- generate_data(100, 10, "Exponential", "Non_Normal")
Weibull <- generate_data(100, 10, "Weibull", "Non_Normal")
Pareto <- generate_data(100, 10, "Pareto", "Non_Normal")
normal_data <- rbind(normal, stdnormal)
non_normal_data <- rbind(lognorma,chisq_data, Weibull, exp_data, Pareto)
# Combine and shuffle data, excluding entropy and tail index columns
data <- rbind(normal_data, non_normal_data)
data <- data[sample(nrow(data)), ]
# Step 3: Split Data into Training and Test Sets
set.seed(123)
trainIndex <- createDataPartition(data$Label, p = .8, list = FALSE, times = 1)
train_data <- data[trainIndex, ]
test_data  <- data[-trainIndex, ]
# Ensure that Label is a factor
train_data$Label <- as.factor(train_data$Label)
test_data$Label <- as.factor(test_data$Label)
# Normalize the training and test data
Normalized_train_data <- normalize_data(train_data)
Normalized_test_data <- normalize_data(test_data)
# Step 4: Build Machine Learning Models
# Logistic Regression Model
log_model <- train(Label ~ ., data = train_data, method = "glm", family = "binomial")
# Required Libraries
rm(list = ls())
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest,lawstat, infotheo, ineq, caret, pROC, ROCR, randomForest, evd, discretization, nnet, ggplot2)
# Clear working environment
rm(list = ls())
# Set directories in local computer
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH/generate_data.R")
calculate_zero_crossing_rate <- function(samples) {
zero_crossings <- sum(diff(samples > 0))
return(zero_crossings / length(samples))
}
calculate_gini_coefficient <- function(samples) {
gini_value <- ineq(samples, type = "Gini")
return(gini_value)
}
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
outliers <- sum(samples < (qnt[1] - H) | samples > (qnt[2] + H))
return(outliers)
}
calculate_shapiro_wilk <- function(samples) {
shapiro_stat <- shapiro.test(samples)$statistic
return(shapiro_stat)
}
# Shapiro-Francia Test
calculate_shapiro_francia <- function(samples) {
result <- tryCatch({
lawstat::sf.test(samples)$statistic
}, error = function(e) {
warning("Shapiro-Francia test failed")
return(NA)
})
return(as.numeric(result))
}
# Liliefors Test (Kolmogorov-Smirnov for estimated parameters)
calculate_lilliefors <- function(samples) {
result <- tryCatch({
nortest::lillie.test(samples)$statistic
}, error = function(e) {
warning("Lilliefors test failed")
return(NA)
})
return(as.numeric(result))
}
# Cramer-von Mises Test
calculate_cramer_von_mises <- function(samples) {
result <- tryCatch({
nortest::cvm.test(samples)$statistic
}, error = function(e) {
warning("Cramer-von Mises test failed")
return(NA)
})
return(as.numeric(result))
}
calculate_features <- function(samples) {
skewness <- e1071::skewness(samples)
kurtosis <- e1071::kurtosis(samples)
jb_stat <- tseries::jarque.bera.test(samples)$statistic
ad_stat <- nortest::ad.test(samples)$statistic
zero_crossing_rate <- calculate_zero_crossing_rate(samples)
gini_coefficient <- calculate_gini_coefficient(samples)
outliers <- calculate_outliers(samples)
shapiro_wilk <- calculate_shapiro_wilk(samples)
shapiro_francia <- calculate_shapiro_francia(samples)
lilliefors_stat <- calculate_lilliefors(samples)
cramer_von_mises <- calculate_cramer_von_mises(samples)
features <- data.frame(
Skewness = skewness,
Kurtosis = kurtosis,
JB_Statistic = as.numeric(jb_stat),
AD_Statistic = as.numeric(ad_stat),
Zero_Crossing_Rate = zero_crossing_rate,
Gini_Coefficient = gini_coefficient,
Outliers = outliers,
Shapiro_Wilk = as.numeric(shapiro_wilk),
# Shapiro_Francia = shapiro_francia,
Liliefors = lilliefors_stat,
Cramer_Von_Mises = cramer_von_mises
)
return(features)
}
# Min-Max normalization function
min_max_normalize <- function(data) {
return((data - mean(data)) / (sd(data)))
}
# Normalize all numeric columns except the Label column
normalize_data <- function(data) {
numeric_features <- data[sapply(data, is.numeric)]  # Extract numeric features
normalized_features <- as.data.frame(lapply(numeric_features, min_max_normalize))
normalized_data <- cbind(normalized_features, Label = data$Label)  # Re-attach the Label column
return(normalized_data)
}
# Step 2: Prepare Data for Machine Learning Models
generate_data <- function(n_samples, n_per_sample, dist = "normal", label) {
data <- do.call(rbind, lapply(1:n_samples, function(x) {
samples <- generate_samples(n_per_sample, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
# Generate data
set.seed(123)
normal <- generate_data(500, 10, "normal", "Normal")
stdnormal <- generate_data(500, 10, "Standard Normal", "Normal")
lognorma <- generate_data(100, 10,  "LogNormal", "Non_Normal")
chisq_data <- generate_data(100, 10, "Chi-Square", "Non_Normal")
exp_data <- generate_data(100, 10, "Exponential", "Non_Normal")
Weibull <- generate_data(100, 10, "Weibull", "Non_Normal")
Pareto <- generate_data(100, 10, "Pareto", "Non_Normal")
normal_data <- rbind(normal, stdnormal)
non_normal_data <- rbind(lognorma,chisq_data, Weibull, exp_data, Pareto)
# Combine and shuffle data, excluding entropy and tail index columns
data <- rbind(normal_data, non_normal_data)
data <- data[sample(nrow(data)), ]
# Step 3: Split Data into Training and Test Sets
set.seed(123)
trainIndex <- createDataPartition(data$Label, p = .8, list = FALSE, times = 1)
train_data <- data[trainIndex, ]
test_data  <- data[-trainIndex, ]
# Ensure that Label is a factor
train_data$Label <- as.factor(train_data$Label)
test_data$Label <- as.factor(test_data$Label)
# Normalize the training and test data
Normalized_train_data <- normalize_data(train_data)
Normalized_test_data <- normalize_data(test_data)
# Step 4: Build Machine Learning Models
# Logistic Regression Model
log_model <- train(Label ~ ., data = train_data, method = "glm", family = "binomial")
# Required Libraries
rm(list = ls())
if(!require("pacman")) install.packages("pacman")
pacman::p_load(e1071, tseries, nortest,lawstat, infotheo, ineq, caret, pROC, ROCR, randomForest, evd, discretization, nnet, ggplot2)
# Clear working environment
rm(list = ls())
# Set directories in local computer
setwd("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH")
source("~/Desktop/OSU/Research/Pretest-Simulation/Simulation/MACHINE LEARNING APPROACH/generate_data.R")
calculate_zero_crossing_rate <- function(samples) {
zero_crossings <- sum(diff(samples > 0))
return(zero_crossings / length(samples))
}
calculate_gini_coefficient <- function(samples) {
gini_value <- ineq(samples, type = "Gini")
return(gini_value)
}
calculate_outliers <- function(samples) {
qnt <- quantile(samples, probs = c(0.25, 0.75))
H <- 1.5 * IQR(samples)
outliers <- sum(samples < (qnt[1] - H) | samples > (qnt[2] + H))
return(outliers)
}
calculate_shapiro_wilk <- function(samples) {
shapiro_stat <- shapiro.test(samples)$statistic
return(shapiro_stat)
}
# Shapiro-Francia Test
calculate_shapiro_francia <- function(samples) {
result <- tryCatch({
lawstat::sf.test(samples)$statistic
}, error = function(e) {
warning("Shapiro-Francia test failed")
return(NA)
})
return(as.numeric(result))
}
# Liliefors Test (Kolmogorov-Smirnov for estimated parameters)
calculate_lilliefors <- function(samples) {
result <- tryCatch({
nortest::lillie.test(samples)$statistic
}, error = function(e) {
warning("Lilliefors test failed")
return(NA)
})
return(as.numeric(result))
}
# Cramer-von Mises Test
calculate_cramer_von_mises <- function(samples) {
result <- tryCatch({
nortest::cvm.test(samples)$statistic
}, error = function(e) {
warning("Cramer-von Mises test failed")
return(NA)
})
return(as.numeric(result))
}
calculate_features <- function(samples) {
skewness <- e1071::skewness(samples)
kurtosis <- e1071::kurtosis(samples)
jb_stat <- tseries::jarque.bera.test(samples)$statistic
ad_stat <- nortest::ad.test(samples)$statistic
zero_crossing_rate <- calculate_zero_crossing_rate(samples)
gini_coefficient <- calculate_gini_coefficient(samples)
outliers <- calculate_outliers(samples)
shapiro_wilk <- calculate_shapiro_wilk(samples)
shapiro_francia <- calculate_shapiro_francia(samples)
lilliefors_stat <- calculate_lilliefors(samples)
cramer_von_mises <- calculate_cramer_von_mises(samples)
features <- data.frame(
Skewness = skewness,
Kurtosis = kurtosis,
JB_Statistic = as.numeric(jb_stat),
AD_Statistic = as.numeric(ad_stat),
Zero_Crossing_Rate = zero_crossing_rate,
Gini_Coefficient = gini_coefficient,
Outliers = outliers,
Shapiro_Wilk = as.numeric(shapiro_wilk),
# Shapiro_Francia = shapiro_francia,
Liliefors = lilliefors_stat,
Cramer_Von_Mises = cramer_von_mises
)
return(features)
}
# Min-Max normalization function
min_max_normalize <- function(data) {
return((data - mean(data)) / (sd(data)))
}
# Normalize all numeric columns except the Label column
normalize_data <- function(data) {
numeric_features <- data[sapply(data, is.numeric)]  # Extract numeric features
normalized_features <- as.data.frame(lapply(numeric_features, min_max_normalize))
normalized_data <- cbind(normalized_features, Label = data$Label)  # Re-attach the Label column
return(normalized_data)
}
# Step 2: Prepare Data for Machine Learning Models
generate_data <- function(n_samples, n_per_sample, dist = "normal", label) {
data <- do.call(rbind, lapply(1:n_samples, function(x) {
samples <- generate_samples(n_per_sample, dist)
features <- calculate_features(samples)
features$Label <- label
return(features)
}))
return(data)
}
# Generate data
set.seed(123)
normal <- generate_data(500, 10, "normal", "Normal")
stdnormal <- generate_data(500, 10, "Standard Normal", "Normal")
lognorma <- generate_data(100, 10,  "LogNormal", "Non_Normal")
chisq_data <- generate_data(100, 10, "Chi-Square", "Non_Normal")
exp_data <- generate_data(100, 10, "Exponential", "Non_Normal")
Weibull <- generate_data(100, 10, "Weibull", "Non_Normal")
Pareto <- generate_data(100, 10, "Pareto", "Non_Normal")
normal_data <- rbind(normal, stdnormal)
non_normal_data <- rbind(chisq_data, Weibull, exp_data, Pareto)
# Combine and shuffle data, excluding entropy and tail index columns
data <- rbind(normal_data, non_normal_data)
data <- data[sample(nrow(data)), ]
# Step 3: Split Data into Training and Test Sets
set.seed(123)
trainIndex <- createDataPartition(data$Label, p = .8, list = FALSE, times = 1)
train_data <- data[trainIndex, ]
test_data  <- data[-trainIndex, ]
# Ensure that Label is a factor
train_data$Label <- as.factor(train_data$Label)
test_data$Label <- as.factor(test_data$Label)
# Normalize the training and test data
Normalized_train_data <- normalize_data(train_data)
Normalized_test_data <- normalize_data(test_data)
# Step 4: Build Machine Learning Models
# Logistic Regression Model
log_model <- train(Label ~ ., data = train_data, method = "glm", family = "binomial")
