results_power <- list()
results_type1 <- list()
# track runtime
timing <- list()
# store all p-values
pval_storage <- list(H0 = list(), H1 = list())
# for each ds test method
for (method in ds_test_methods) {
cat("Running method:", method, "\n")
# storage for each sample size
pow_vec <- typ1_vec <- numeric(length(sample_sizes))
names(pow_vec) <- names(typ1_vec) <- paste0("n=", sample_sizes)
# store all p-values
pval_storage$H0[[method]] <- vector("list", length(sample_sizes))
pval_storage$H1[[method]] <- vector("list", length(sample_sizes))
# set progress bar
pb <- txtProgressBar(min = 0, max = length(sample_sizes), style = 3)
# start time
t0 <- Sys.time()
for (i in seq_along(sample_sizes)) {
n <- sample_sizes[i]
#alpha_pre <- alpha_optimum
rej_H0 <- rej_H1 <- 0L
p_H0 <- p_H1 <- numeric(N)
for (sim in seq_len(N)) {
# get parameters
paras_H0 <- get_parameters(n, effect_size = 0.0, dist = dist)
paras_H1 <- get_parameters(n, effect_size = effect_size, dist = dist)
# generate data
dat_H0 <- do.call(gen_data, paras_H0)
dat_H1 <- do.call(gen_data, paras_H1)
if (method == "test_1") { # power & type 1 error p-values for test 1
p0 <- fn_for_ds_test_1(dat_H0)$p.value
p1 <- fn_for_ds_test_1(dat_H1)$p.value
} else if (method == "test_2") {# power & type 1 error p-values for test 2
p0 <- fn_for_ds_test_2(dat_H0)$p.value
p1 <- fn_for_ds_test_2(dat_H1)$p.value
} else {# type I error for adaptive test
p0 <- ds_test_function(
gen_data = gen_data, get_parameters = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = fn_for_norm_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
paras = paras_H0, alpha = alpha,
norm_test_method = norm_test_method, alpha_pre = alpha_pre, ...
)# power for adaptive test
p1 <- ds_test_function(
gen_data = gen_data, get_parameters = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = fn_for_norm_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
paras = paras_H1, alpha = alpha,
norm_test_method = norm_test_method, alpha_pre = alpha_pre, ...
)
}# store p-values
p_H0[sim] <- p0
p_H1[sim] <- p1
if (p0 < alpha) rej_H0 <- rej_H0 + 1
if (p1 < alpha) rej_H1 <- rej_H1 + 1
}
# calculate power & type I error rates
typ1_vec[i] <- rej_H0 / N
pow_vec[i]  <- rej_H1 / N
pval_storage$H0[[method]][[i]] <- p_H0
pval_storage$H1[[method]][[i]] <- p_H1
setTxtProgressBar(pb, i)
}
close(pb)
timing[[method]] <- as.numeric(difftime(Sys.time(), t0, units = "secs"))
results_type1[[method]] <- typ1_vec
results_power[[method]] <- pow_vec
cat("Method", method, "completed in", round(timing[[method]], 2), "seconds\n")
}
# collect all results by distribution
results_by_dist[[dist]] <- list(
power = results_power,
type1 = results_type1,
pvalues = pval_storage,
timing = timing
)
}
return(results_by_dist)
}
# -----------------------------------------------------------------------------
#                     power & type I error plot function
# -----------------------------------------------------------------------------
plot_power_type1 <- function(combined_power, combined_type1, optimal_alphas, methods = c("test_1", "test_2", "adaptive"), distributions = c("Non-normal", "Normal"), sizes, alpha, filename = NULL, width = 7, height = 6) {
# Setup
colors <- c("red", "blue", "green")
shapes <- c(19, 17, 15)
# Open PDF if file name provided
if (!is.null(filename)) {
pdf(filename, width = width, height = height)
on.exit(dev.off())
}
# Plot layout: 2 plots, 2 plots, 1 legend
layout(matrix(c(1,2,3,4,5,5), nrow = 3, byrow = TRUE), heights = c(1,1,0.25))
op <- par(mar = c(2.6, 3.2, 1.2, 0.8), oma = c(0, 0,1.6,0), mgp = c(1.6, 0.45, 0), tcl = -0.2, cex.axis = 0.75, cex.lab = 0.75)
on.exit(par(op), add = TRUE)
# Plot function
create_plot <- function(data, ylab, main, hline = NULL) {
xlim <- range(sizes) + c(-1, 1) * 0.02 * diff(range(sizes))
plot(NA, xlim = xlim, ylim = c(0, 1.01), xlab = "Sample Size", ylab = ylab, main = main)
for (i in seq_along(methods)) {
lines(data$n, data[[methods[i]]], type = "b", col = colors[i], pch = shapes[i], lwd = 2)
}
if (!is.null(hline)) abline(h = hline, col = "gray50", lty = 2)
}
# Create plots
for (dist in distributions) {
data_power <- subset(combined_power, Distribution == dist)
create_plot(data_power, "Power", paste("Power -", dist), 0.75)
}
for (dist in distributions) {
data_type1 <- subset(combined_type1, Distribution == dist)
ymax <- max(data_type1[methods], na.rm = TRUE)
plot(NA, xlim = range(sizes), ylim = c(0, ymax * 1.02), xlab = "Sample Size", ylab = "P(Type I Error)", main = paste("P(Type I Error) -", dist))
for (i in seq_along(methods)) {
lines(data_type1$n, data_type1[[methods[i]]], type = "b", col = colors[i], pch = shapes[i], lwd = 2)
}
abline(h = alpha, col = "gray50", lty = 3)
}
# Legend
par(mar = c(0.2, 0.2, 0.2, 0.2))
plot.new()
legend("center", legend = methods, col = colors, pch = shapes, lwd = 2, horiz = TRUE, bty = "n", cex = 0.75, title = "Methods")
# Title with optimal alphas
mtext(sprintf("Test Methods Comparison (Optimal alpha: %s)", paste(round(optimal_alphas, 3), collapse = ", ")), side = 3, outer = TRUE, line = 0.35, cex = 0.75)
}
# ----------------------------------------------------------------------------
# calculate power for each effect size
# ----------------------------------------------------------------------------
perform_ds_power_by_effect <- function(fixed_n = 10, effect_sizes = NULL, distributions = c("Non-normal", "Normal"), N = N, alpha = alpha, gen_data = gen_data,
get_parameters = get_parameters, fn_to_get_norm_obj = fn_to_get_norm_obj, fn_for_norm_test = normality_test,
fn_for_ds_test_1 = fn_for_ds_test_1, fn_for_ds_test_2 = fn_for_ds_test_2, norm_test_method = norm_test_method,
ds_test_methods = ds_test_methods, alpha_pre = alpha_pre, ...) {
results_by_dist <- list()
for (dist in distributions) {
cat("Power simulation:", dist, "n =", fixed_n, "\n")
# store results
power_results <- list()
timing <- list()
pvalues <- list(H1 = list())
for (method in ds_test_methods) {
cat("Method:", method, "\n")
power_vec <- numeric(length(effect_sizes))
names(power_vec) <- paste0("d=", effect_sizes)
pvalues$H1[[method]] <- vector("list", length(effect_sizes))
pb <- txtProgressBar(0, length(effect_sizes), style = 3)
start_time <- Sys.time()
for (i in seq_along(effect_sizes)) {
effect <- effect_sizes[i]
rejections <- 0
p_vals <- numeric(N)
for (sim in 1:N) {
paras <- get_parameters(fixed_n, effect_size = effect, dist = dist)
data <- do.call(gen_data, paras)
if (method == "test_1") { # test 1
p_val <- fn_for_ds_test_1(data)$p.value
} else if (method == "test_2") { # test 2
p_val <- fn_for_ds_test_2(data)$p.value
} else {# adaptive test
p_val <- ds_test_function(
gen_data = gen_data, get_parameters = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test = fn_for_norm_test,
fn_for_ds_test_1 = fn_for_ds_test_1,
fn_for_ds_test_2 = fn_for_ds_test_2,
paras = paras, alpha = alpha,
norm_test_method = norm_test_method, alpha_pre = alpha_pre, ...
)
}
p_vals[sim] <- p_val
if (p_val < alpha) rejections <- rejections + 1
}
# calculate power and store all p-values
power_vec[i] <- rejections / N
pvalues$H1[[method]][[i]] <- p_vals
setTxtProgressBar(pb, i)
}
close(pb)
timing[[method]] <- as.numeric(Sys.time() - start_time)
power_results[[method]] <- power_vec
cat("Completed in", round(timing[[method]], 2), "s\n")
}
results_by_dist[[dist]] <- list(
power = power_results,
pvalues = pvalues,
timing = timing
)
}
results_by_dist
}
# -----------------------------------------------------------------------
# plot power against effect sizes
# ----------------------------------------------------------------------
plot_power_by_effect_size <- function(power_results,
distributions = c("Non-normal", "Normal"),
ds_test_methods = c("test_1", "test_2", "adaptive"),
effect_sizes,
alpha_pre = 0.05) {
colors <- c("red", "blue", "green")
shapes <- c(19, 17, 15)
# Save and restore graphics settings
op <- par(no.readonly = TRUE)
on.exit(par(op))
# Layout: 2 plots on top, legend row at bottom
layout(
matrix(c(1, 2,
3, 3), nrow = 2, byrow = TRUE),
heights = c(0.8, 0.2)
)
# Margins for the two main plots
par(mar = c(4, 4, 2, 1), oma = c(0, 0, 2, 0))
for (dist in distributions) {
dist_data <- subset(power_results, Distribution == dist)
plot(NA,
xlim = range(effect_sizes),
ylim = c(0, 1),
xlab = "Effect Size",
ylab = "Power",
main = dist,
cex.main = 0.9)
for (i in seq_along(ds_test_methods)) {
method <- ds_test_methods[i]
if (method %in% names(dist_data)) {
y <- dist_data[[method]]
lines(effect_sizes, y, col = colors[i], lwd = 2)
points(effect_sizes, y, pch = shapes[i], col = colors[i])
}
}
abline(v = 0.5, col = "gray50", lty = 2)
}
# Overall title
mtext(sprintf("Power vs Effect Size | pre-test = %.3f", alpha_pre),
outer = TRUE, side = 3, line = 0, cex = 0.9)
# Legend panel (bottom row)
par(mar = c(0, 0, 0, 0))
plot.new()
legend("center",
legend = ds_test_methods,
title = "Method",
col = colors,
lwd = 2,
pch = shapes,
bty = "n",
horiz = TRUE,
cex = 0.9)
}
# =============================================================================
#                             RUN SIMULATIONS                                 #
# =============================================================================
run_simulation <- function(n, N, Nsim, test_type, distributions, tol_pos , alpha_pre) {
# create results folder
if (!dir.exists("results")) dir.create("results")
## Keep a small results
results <- list(
params  = list(N = N, Nsim = Nsim, test_type = test_type, distributions = distributions , alpha_pre = alpha_pre),
objects = list(),
files   = list()
)
# define run control parameters
sample_size       <- c(10, 20, 30, 40, 50)
test_alpha        <- 0.05
alpha             <- 0.05
select_norm_test  <- "SW"
effect_size       <- 0.5
norm_alpha_pre    <- seq(from = 0.0, to = 1, by = 0.05)
alpha_pretest     <- seq(from = 0.009, to = 1, by = 0.0025)
effect_sizes      <- c(0.0, 0.1, 0.2, 0.3, 0.5, 0.8, 1.0)
sig_levels        <- seq(from = 0.005, to = 1, by = 0.005)
ds_test_methods   <- c("test_1", "test_2", "adaptive")
norm_test         <- c("SW", "SF", "KS", "JB","SKEW", "DAP", "AD", "CVM")
## ROC data for normality tests
roc_pval_ds_test <- fn_for_roc_curve_for_norm_test(
n = 10,
alpha_pretest = norm_alpha_pre,
H1_dist = distributions[1],
tests = norm_test,
Nsim = Nsim
)
# store results
results$objects$roc_pval_ds_test <- roc_pval_ds_test
## Plot ROC using selected tests
pdf(paste0("results/", test_type, "_test_norm_roc_curve.pdf"), width = 6, height = 5)
selected_tests <- c("SW", "SF", "KS", "JB","SKEW", "DAP", "AD", "CVM")
plot_norm_roc_curve(
FPR = roc_pval_ds_test$FPR,
TPR = roc_pval_ds_test$TPR,
tests_to_plot = selected_tests,
alpha = roc_pval_ds_test$alpha,
dist_name = distributions[1]
)
dev.off()
# Trade-off analysis (power & Type I error)
analysis_ds_tests <- perform_analysis(
Nsim = Nsim,
n = 10,
distributions = distributions,
effect_size = effect_size,
test = select_norm_test,
alpha_pretest = alpha_pretest,
test_alpha = test_alpha
)
# store results
results$objects$analysis_ds_tests <- analysis_ds_tests
# Calculate metrics
metrics <- compute_roc_metrics(
error_ds_test = analysis_ds_tests$error_ds_test,
power_ds_test = analysis_ds_tests$power_ds_test,
test_alpha    = test_alpha
)
# store results
results$objects$metrics <- metrics
# Capture the trade off result to get alpha_star
pdf(paste0("results/", test_type, "_test_tradeoff.pdf"), width = 6, height = 5)
tradeoff_result <- plot_power_error_tradeoff(
distributions = distributions,
alpha_pretest = alpha_pretest,
metrics = metrics,
tol_pos = tol_pos
)
dev.off()
# Save the trade off result and extract alpha_star
results$objects$tradeoff_result <- tradeoff_result
alpha_star <- tradeoff_result$alpha_star
results$objects$alpha_star <- alpha_star
cat("=== Selected alpha_star from tradeoff analysis:", round(alpha_star, 4), "===\n")
## Generate data for Power vs Type I error like plot
roc_data <- power_vs_error_roc_data(
N = N,
n = 10,
distributions = distributions,
effect_size = effect_size,
alpha_pretest = alpha_star,
sig_levels = sig_levels
)
# store results
results$objects$roc_data <- roc_data
#  Power vs Type I error ROC-like plots
pdf(paste0("results/", test_type, "_test_power_error_roc.pdf"), width = 6, height = 5)
power_vs_error_ROC_curve(roc_data = roc_data)
dev.off()
# -------------------------------------------------------
# ------- Main simulation: power/Type I error analysis --
# -------------------------------------------------------
sim_output <- perform_ds_func(
sample_sizes       = sample_size,
distributions      = distributions,
N                  = N,
alpha              = alpha,
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = normality_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
norm_test_method   = select_norm_test,
ds_test_methods    = ds_test_methods,
effect_size        = effect_size,
alpha_pre          = alpha_star
)
# store results
results$objects$sim_output <- sim_output
## Per-distribution AUCs (printed and saved)
all_auc_tables <- list()
for (dist in names(sim_output)) {
cat("\n\n=== Results for", dist, "distribution ===\n")
dist_results  <- sim_output[[dist]]
power_results <- dist_results$power
type1_results <- dist_results$type1
# power
power_df <- list(
test_1    = power_results$test_1,
test_2 = power_results$test_2,
adaptive      = power_results$adaptive
)
# type I error
TypeI_error_df <- list(
test_1    = type1_results$test_1,
test_2 = type1_results$test_2,
adaptive      = type1_results$adaptive
)
# calculate aucs
auc_power <- sapply(power_df, function(y) compute_area(sample_size, y))
auc_type1 <- sapply(TypeI_error_df, function(y) compute_area(sample_size, y))
# store in a dataframe
auc_table <- data.frame(
Method    = names(auc_power),
AUC_Power = unname(auc_power),
AUC_TypeI = unname(auc_type1),
row.names = NULL
)
all_auc_tables[[dist]] <- auc_table
print(auc_table, digits = 4)
}
# store results
results$objects$auc_tables <- all_auc_tables
## Combine across distributions and plot
all_power_results <- list()
all_type1_results <- list()
for (dist in names(sim_output)) {
dist_results <- sim_output[[dist]]
power_df <- data.frame(
n = sample_size,
test_1    = dist_results$power$test_1,
test_2 = dist_results$power$test_2,
adaptive      = dist_results$power$adaptive,
Distribution  = dist
)
type1_df <- data.frame(
n = sample_size,
test_1    = dist_results$type1$test_1,
test_2 = dist_results$type1$test_2,
adaptive      = dist_results$type1$adaptive,
Distribution  = dist
)
all_power_results[[dist]] <- power_df
all_type1_results[[dist]] <- type1_df
}
combined_power <- do.call(rbind, all_power_results)
combined_type1 <- do.call(rbind, all_type1_results)
results$objects$combined_power_n <- combined_power
results$objects$combined_type1_n <- combined_type1
#
pdf(paste0("results/", test_type, "_power_and_error_test.pdf"), width = 6, height = 5)
plot_power_type1(
combined_power  = combined_power,
combined_type1  = combined_type1,
optimal_alphas  = alpha_star,
methods         = ds_test_methods,
distributions   = distributions,
sizes           = sample_size,
alpha           = alpha
)
dev.off()
# -----------------------------------------------------------
## Effect-size power curves
# ----------------------------------------------------------
sample_size_es <- 10
sim_power_list <- list()
for (n in sample_size_es) {
sim_power_list[[as.character(n)]] <- perform_ds_power_by_effect(
fixed_n            = n,
effect_sizes       = effect_sizes,
distributions      = distributions,
N                  = N,
alpha              = alpha,
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = normality_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
norm_test_method   = select_norm_test,
ds_test_methods    = ds_test_methods,
alpha_pre          = alpha_star
)
}
# store results
results$objects$sim_power_list <- sim_power_list
all_power_df <- list()
for (n in names(sim_power_list)) {
for (dist in names(sim_power_list[[n]])) {
pr <- sim_power_list[[n]][[dist]]$power
all_power_df[[paste(dist, n, sep = "_")]] <- data.frame(
d    = effect_sizes,
test_1    = pr$test_1,
test_2 = pr$test_2,
adaptive      = pr$adaptive,
Distribution  = dist,
n             = as.integer(n)
)
}
}
# store results
combined_power_effect <- do.call(rbind, all_power_df)
results$objects$combined_power_effect <- combined_power_effect
# plot power vs effect sizes
pdf(paste0("results/", test_type, "_power_by_effect.pdf"), 6, 4)
plot_power_by_effect_size(
power_results   = combined_power_effect,
distributions   = distributions,
ds_test_methods = ds_test_methods,
effect_sizes    = effect_sizes,
alpha_pre       = alpha_star
)
dev.off()
## ---------- Save compact results + full workspace ----------
save(results, file = paste0("results/", test_type, "_results.RData"))
save.image(paste0("results/", test_type, "_workspace.RData"))
# Final summary
cat("\n=== SIMULATION COMPLETE ===\n")
cat("Selected alpha_star used throughout:", round(alpha_star, 4), "\n")
cat("Tradeoff feasibility:", if(tradeoff_result$feasible) "FEASIBLE" else "FALLBACK", "\n")
cat("Worst-case positive inflation:", round(tradeoff_result$worst_case_positive_inflation, 6), "\n")
cat("Inflation values (Normal, Non-normal):",
round(tradeoff_result$inflation_normal, 6), ",",
round(tradeoff_result$inflation_non_normal, 6), "\n")
invisible(results)
}
## Example call
run_simulation(
N = 1e2,
Nsim = 1e2,
test_type = "two_sample_t_vs_wilcoxon",
distributions = c("exponential", "normal"),
tol_pos = 1e-2,
alpha_pre = 0.05
)
