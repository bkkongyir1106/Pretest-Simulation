#'         each test, along with the simulation parameters.
#'
#' @examples
#' # Simulate Type I error under normal data
#' results_normal <- simulate_t_test_error(
#'   n_iter = 1000, n1 = 20, n2 = 20, mu_diff = 0,
#'   distr = "normal", seed = 123
#' )
#' print(results_normal)
#'
#' # Simulate Power under lognormal data
#' results_lognormal <- simulate_t_test_error(
#'   n_iter = 1000, n1 = 30, n2 = 30, mu_diff = 0.5,
#'   distr = "lognormal", seed = 456
#' )
#' print(results_lognormal)
simulate_t_test_error <- function(n_iter = 1000, n1, n2, mu_diff, distr, sd_ratio = 1, alpha = 0.05, seed = NULL) {
if (!is.null(seed)) set.seed(seed)
# Initialize counters
reject_standard <- 0
reject_robust <- 0
# Define parameters for distributions to ensure a comparable scale
# We aim for a mean near 1 for the base group to avoid negative values for Box-Cox
base_mean <- 10
base_sd <- 2
mu1 <- base_mean
mu2 <- base_mean + mu_diff
sd1 <- base_sd
sd2 <- base_sd * sd_ratio
for (i in 1:n_iter) {
# Generate data based on the specified distribution
if (distr == "normal") {
group1 <- rnorm(n1, mean = mu1, sd = sd1)
group2 <- rnorm(n2, mean = mu2, sd = sd2)
} else if (distr == "lognormal") {
# Parameterize lognormal using meanlog and sdlog derived from desired mean/sd
meanlog1 <- log(mu1^2 / sqrt(sd1^2 + mu1^2))
sdlog1 <- sqrt(log(1 + (sd1^2 / mu1^2)))
meanlog2 <- log(mu2^2 / sqrt(sd2^2 + mu2^2))
sdlog2 <- sqrt(log(1 + (sd2^2 / mu2^2)))
group1 <- rlnorm(n1, meanlog = meanlog1, sdlog = sdlog1)
group2 <- rlnorm(n2, meanlog = meanlog2, sdlog = sdlog2)
} else if (distr == "exponential") {
rate1 <- 1 / mu1
rate2 <- 1 / mu2
group1 <- rexp(n1, rate = rate1)
group2 <- rexp(n2, rate = rate2)
} else if (distr == "gamma") {
# Shape = (mean/sd)^2, Rate = mean/sd^2
shape1 <- (mu1 / sd1)^2
rate1 <- mu1 / sd1^2
shape2 <- (mu2 / sd2)^2
rate2 <- mu2 / sd2^2
group1 <- rgamma(n1, shape = shape1, rate = rate1)
group2 <- rgamma(n2, shape = shape2, rate = rate2)
} else if (distr == "weibull") {
# Parameterize Weibull with scale (lambda) and shape (k)
# Using approx: scale ~ mean / gamma(1 + 1/shape), often shape=2 is a reasonable skew
k <- 2 # shape parameter
lambda1 <- mu1 / gamma(1 + 1/k)
lambda2 <- mu2 / gamma(1 + 1/k)
group1 <- rweibull(n1, shape = k, scale = lambda1)
group2 <- rweibull(n2, shape = k, scale = lambda2)
} else {
stop("Distribution not supported. Use 'normal', 'lognormal', 'exponential', 'gamma', or 'weibull'.")
}
# Perform the standard t-test (assuming unequal variances by default)
tryCatch({
test_standard <- t.test(group1, group2)
p_standard <- test_standard$p.value
}, error = function(e) {
p_standard <<- NA # Assign NA if test fails (e.g., all values identical)
})
# Perform the robust t-test (our function from before)
tryCatch({
test_robust <- robust_t_test(group1, group2, alpha = 0.05)
p_robust <- test_robust$p.value
}, error = function(e) {
p_robust <<- NA
})
# Count rejections, handling potential NAs
if (!is.na(p_standard) && p_standard < alpha) {
reject_standard <- reject_standard + 1
}
if (!is.na(p_robust) && p_robust < alpha) {
reject_robust <- reject_robust + 1
}
}
# Calculate proportions (Error Rate or Power)
power_standard <- reject_standard / n_iter
power_robust <- reject_robust / n_iter
# Return results as a tidy data frame
results_df <- data.frame(
Distribution = distr,
Sample_Size = paste0("n1=", n1, ", n2=", n2),
True_Mean_Difference = mu_diff,
SD_Ratio = sd_ratio,
Nominal_Alpha = alpha,
Iterations = n_iter,
Power_Standard_T = power_standard,
Power_Robust_T = power_robust
)
return(results_df)
}
# Load required libraries
library(ggplot2)
library(dplyr)
library(purrr)
library(tidyr)
# --- Define the simulation scenarios ---
scenarios <- expand.grid(
n = c(15, 30, 50),           # Sample sizes per group
mu_diff = c(0, 0.5, 1.0),    # 0 for Type I error, >0 for Power
distr = c("normal", "lognormal", "exponential"), # Distributions to test
sd_ratio = c(1, 2),          # Equal and unequal variance
stringsAsFactors = FALSE
)
# --- Run the simulations (This will take a while!) ---
n_iterations <- 2000 # Use at least 2000-5000 for stable estimates
# Use map2 (or a for loop) to iterate over scenarios
simulation_results <- purrr::map2_df(
1:nrow(scenarios),
scenarios$distr,
function(i, d) {
cat("Running scenario", i, "of", nrow(scenarios), "\n")
simulate_t_test_error(
n_iter = n_iterations,
n1 = scenarios$n[i],
n2 = scenarios$n[i], # Using equal group sizes for simplicity
mu_diff = scenarios$mu_diff[i],
distr = d,
sd_ratio = scenarios$sd_ratio[i],
seed = i # Use index as seed for reproducibility
)
}
)
# --- Define the simulation scenarios ---
scenarios <- expand.grid(
n = c(15, 30, 50),           # Sample sizes per group
mu_diff = c(0, 0.5, 1.0),    # 0 for Type I error, >0 for Power
distr = c("normal", "lognormal", "exponential"), # Distributions to test
sd_ratio = c(1, 2),          # Equal and unequal variance
stringsAsFactors = FALSE
)
# --- Run the simulations (This will take a while!) ---
n_iterations <- 100 # Use at least 2000-5000 for stable estimates
# Use map2 (or a for loop) to iterate over scenarios
simulation_results <- purrr::map2_df(
1:nrow(scenarios),
scenarios$distr,
function(i, d) {
cat("Running scenario", i, "of", nrow(scenarios), "\n")
simulate_t_test_error(
n_iter = n_iterations,
n1 = scenarios$n[i],
n2 = scenarios$n[i], # Using equal group sizes for simplicity
mu_diff = scenarios$mu_diff[i],
distr = d,
sd_ratio = scenarios$sd_ratio[i],
seed = i # Use index as seed for reproducibility
)
}
)
# 1. Create a summary data frame for plotting
plot_data <- simulation_results %>%
mutate(
Scenario_Type = ifelse(True_Mean_Difference == 0, "Type I Error", "Power"),
Test_Type = "Standard" # Temporary column
)
# Duplicate rows to have both tests in a long format
plot_data_long <- bind_rows(
plot_data %>% mutate(Test_Type = "Standard", Power = Power_Standard_T),
plot_data %>% mutate(Test_Type = "Robust (Box-Cox)", Power = Power_Robust_T)
) %>% select(-Power_Standard_T, -Power_Robust_T)
# 2. Plot Type I Error Rates (mu_diff = 0)
type1_error_data <- plot_data_long %>% filter(Scenario_Type == "Type I Error")
type1_plot <- ggplot(type1_error_data, aes(x = Distribution, y = Power, fill = Test_Type)) +
geom_col(position = position_dodge(), alpha = 0.8) +
geom_hline(yintercept = 0.05, linetype = "dashed", color = "red") +
facet_grid(Sample_Size ~ SD_Ratio, labeller = label_both) +
labs(title = "Type I Error Rate Comparison",
subtitle = "Dashed red line indicates nominal alpha = 0.05",
y = "Type I Error Rate",
x = "Data Distribution",
fill = "Test Method") +
theme_minimal() +
theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(type1_plot)
# 3. Plot Statistical Power (mu_diff > 0) for a specific case, e.g., n=30, sd_ratio=1
power_data <- plot_data_long %>%
filter(Scenario_Type == "Power", Sample_Size == "n1=30, n2=30", SD_Ratio == 1)
power_plot <- ggplot(power_data, aes(x = as.factor(True_Mean_Difference), y = Power, color = Test_Type, group = Test_Type)) +
geom_line(size = 1.2) +
geom_point(size = 3) +
facet_wrap(~Distribution, scales = "free_y") +
labs(title = "Statistical Power Comparison (n=30 per group, Equal Variance)",
y = "Power (1 - Beta)",
x = "True Difference in Means",
color = "Test Method") +
theme_minimal()
print(power_plot)
# 4. Create a summary table for the dissertation appendix
summary_table <- plot_data_long %>%
group_by(Distribution, Sample_Size, True_Mean_Difference, SD_Ratio, Test_Type) %>%
summarise(Mean_Power = mean(Power, na.rm = TRUE),
.groups = 'drop') %>%
arrange(Distribution, Sample_Size, True_Mean_Difference, SD_Ratio, Test_Type)
print(summary_table, n = 30)
# Load helper source files
source("~/Desktop/OSU/Research/Pretest-Simulation/functions/User_defined_functions.R")
setwd("~/Desktop/OSU/Research/Pretest-Simulation/User_interface/User_framework_ShinyApp")
FilePath <- "~/Desktop/OSU/Research/Research/user_framework/ShinyApp/sample_test_functions/t_test_functions/"
source(paste0(FilePath, "gen_data.R"))
source(paste0(FilePath, "get_parameters.R"))
source(paste0(FilePath, "fn_get_norm_obj.R"))
source(paste0(FilePath, "fn_for_ds_test_1.R"))
source(paste0(FilePath, "fn_for_ds_test_2.R"))
# -----------------------------------------------------------------
#-------------- Define all other functions here -------------------
# -----------------------------------------------------------------
#---------- function for computing auc ----------------------------
compute_area <- function(sample_sizes, y) {
if (is.null(y) || length(y) != length(sample_sizes)) return(NA_real_)
sum(diff(sample_sizes) * (head(y, -1) + tail(y, -1)) / 2) /
(max(sample_sizes) - min(sample_sizes))
}
# --------------------------------------------------------
# ---------- General Normality check function ------------
# --------------------------------------------------------
normality_test <- function(data, test = "SW", alpha = 0.05) {
pvals <- NULL
all_normality_satisfied <- NULL
# Case 1: Single numeric vector
if (is.numeric(data) && is.atomic(data) && is.null(dim(data))) {
pval <- generate_tests(data, test = test)$p.value
pvals <- pval
all_normality_satisfied <- pval > alpha
}
# Case 2: List of numeric vectors
else if (is.list(data) && !is.data.frame(data)) {
pvals <- sapply(data, function(sample) {
generate_tests(sample, test = test)$p.value
})
names(pvals) <- names(data) %||% paste0("Sample", seq_along(pvals))
all_normality_satisfied <- all(pvals > alpha)
}
# Case 3: Wide-format data frame
else if (is.data.frame(data) && all(sapply(data, is.numeric))) {
pvals <- sapply(as.list(data), function(sample) generate_tests(sample, test = test)$p.value)
names(pvals) <- names(data)
all_normality_satisfied <- all(pvals > alpha)
}
# Case 4: Long-format with group labels
else if ((is.data.frame(data) || is.matrix(data)) && ncol(data) >= 2) {
# We assume first column is group, second is value
grouped_samples <- split(data[[2]], data[[1]])
pvals <- sapply(grouped_samples, function(sample) {
generate_tests(sample, test = test)$p.value
})
all_normality_satisfied <- all(pvals > alpha)
}
else {
stop("Unsupported input type: must be numeric vector, list of vectors, or grouped data.")
}
return(list(p_values = pvals, normality_satisfied = all_normality_satisfied))
}
# ------------------------------------------------------------------------------
# ------------------ Downstream Test function -------------------------------
# ------------------------------------------------------------------------------
ds_test_function <- function(
gen_data = gen_data,
get_parameters = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test = normality_test,
fn_for_ds_test_1 = fn_for_ds_test_1,
fn_for_ds_test_2 = fn_for_ds_test_2,
paras            = NULL,
alpha = 0.05,
norm_test_method = "SW",
...
) {
# generate dataset
data <- if (!is.null(paras)) do.call(gen_data, paras) else gen_data()
# get normality test object
normality_test_object     <- fn_to_get_norm_obj(data)
normality_test_pval_ds_test  <- fn_for_norm_test(data = normality_test_object, test = norm_test_method, alpha = alpha)
# choose ds test method based normality test results
if (isTRUE(normality_test_pval_ds_test$normality_satisfied)) {
ds_test <- fn_for_ds_test_1(data)
} else {
ds_test<- fn_for_ds_test_2(data, ...)
}
return(ds_test$p.value)
}
# ----------------- Power/Type I error Analysis Test Function ---------------
perform_ds_func <- function(
sample_sizes       = c(10, 20, 30, 40, 50),
Nsim               = 1e3,
alpha              = 0.05,
n_boot             = NULL,
gen_data           = gen_data,
get_parameters     = function(n) list(n = n),
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = fn_for_norm_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
norm_test_method   = "SW",
ds_test_methods    = c("parametric", "nonparametric", "adaptive"),
effect_size        = NULL,
...
) {
ds_test_methods <- match.arg(ds_test_methods, several.ok = TRUE)
results <- list()
timing <- list()  # To store timing information
for (method in ds_test_methods) {
cat("Running method:", method, "\n")
ds_test_results <- numeric(length(sample_sizes))
names(ds_test_results) <- paste0("n=", sample_sizes)
pb <- txtProgressBar(min = 0, max = length(sample_sizes), style = 3)
start_time <- Sys.time()  # Start timer for this method
for (i in seq_along(sample_sizes)) {
n <- sample_sizes[i]
rejections <- 0
for (sim in seq_len(Nsim)) {
paras <- get_parameters(n, effect_size = effect_size)
data <- if (!is.null(paras)) do.call(gen_data, paras) else gen_data()
p_value <- tryCatch({
if (method == "adaptive") {
ds_test_function(
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_to_get_norm_obj,
fn_for_norm_test   = fn_for_norm_test,
fn_for_ds_test_1   = fn_for_ds_test_1,
fn_for_ds_test_2   = fn_for_ds_test_2,
paras              = paras,
alpha              = alpha,
norm_test_method   = norm_test_method,
...
)
} else if (method == "parametric") {
fn_for_ds_test_1(data)$p.value
} else {
fn_for_ds_test_2(data, ...)$p.value
}
}, error = function(e) NA)
if (!is.na(p_value) && p_value < alpha) {
rejections <- rejections + 1
}
}
ds_test_results[i] <- rejections / Nsim
setTxtProgressBar(pb, i)
}
close(pb)
end_time <- Sys.time()  # End timer for this method
timing[[method]] <- as.numeric(difftime(end_time, start_time, units = "secs"))
results[[method]] <- ds_test_results
cat("Method", method, "completed in", round(timing[[method]], 2), "seconds\n")
}
return(list(results = results, timing = timing))
}
# -------------------------------------------------------
# ------- Main simulation: power/Type I error analysis --
Nsim <- 1e5
sizes <- c(10, 20, 30, 40, 50)
Nsim <- Nsim
alpha <- 0.05
norm_test <- "SW"
effect_size =  0.5
# Give the functions names
gen_data        <- gen_data
get_parameters  <- get_parameters
fn_get_norm_obj <- fn_get_norm_obj
fn_ds_test_1    <- fn_for_ds_test_1
fn_ds_test_2    <- fn_for_ds_test_2
# test methods
ds_test_methods <- c("parametric", "nonparametric", "adaptive")
# run test
sim_output <- perform_ds_func(
sample_sizes       = sizes,
Nsim               = Nsim,
alpha              = alpha,
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_get_norm_obj,
fn_for_norm_test   = normality_test,
fn_for_ds_test_1   = fn_ds_test_1,
fn_for_ds_test_2   = fn_ds_test_2,
norm_test_method   = norm_test,
ds_test_methods    = ds_test_methods,
effect_size        = effect_size
)
# -------------------------------------------------------
# ------- Main simulation: power/Type I error analysis --
Nsim <- 1e3
sizes <- c(10, 20, 30, 40, 50)
Nsim <- Nsim
alpha <- 0.05
norm_test <- "SW"
effect_size =  0.5
# Give the functions names
gen_data        <- gen_data
get_parameters  <- get_parameters
fn_get_norm_obj <- fn_get_norm_obj
fn_ds_test_1    <- fn_for_ds_test_1
fn_ds_test_2    <- fn_for_ds_test_2
# test methods
ds_test_methods <- c("parametric", "nonparametric", "adaptive")
# run test
sim_output <- perform_ds_func(
sample_sizes       = sizes,
Nsim               = Nsim,
alpha              = alpha,
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_get_norm_obj,
fn_for_norm_test   = normality_test,
fn_for_ds_test_1   = fn_ds_test_1,
fn_for_ds_test_2   = fn_ds_test_2,
norm_test_method   = norm_test,
ds_test_methods    = ds_test_methods,
effect_size        = effect_size
)
# Extract results and timing
ds_test_results <- sim_output$results
timing_results <- sim_output$timing
# Access individual test results
ds_test1_results  <- ds_test_results$parametric
ds_test2_results <- ds_test_results$nonparametric
ds_adaptive_results  <- ds_test_results$adaptive
auc_df <- data.frame(
Method = c("Test 1(t_test)", "Test 2(Wilcoxon)", "Test 3(Adaptive)"),
AUC = c(
compute_area(sizes, ds_test1_results),
compute_area(sizes, ds_test2_results),
compute_area(sizes, ds_adaptive_results)
),
Runtime = c(
timing_results$parametric,
timing_results$nonparametric,
timing_results$adaptive
)
)
results <- list(
sizes = sizes,
ds_test1_results = ds_test1_results,
ds_test2_results = ds_test2_results,
ds_adaptive_results = ds_adaptive_results,
auc = auc_df,
timing = timing_results
)
auc_df
# -------------------------------------------------------
# ------- Main simulation: power/Type I error analysis --
Nsim <- 1e4
sizes <- c(10, 20, 30, 40, 50)
Nsim <- Nsim
alpha <- 0.05
norm_test <- "SW"
effect_size =  0.5
# Give the functions names
gen_data        <- gen_data
get_parameters  <- get_parameters
fn_get_norm_obj <- fn_get_norm_obj
fn_ds_test_1    <- fn_for_ds_test_1
fn_ds_test_2    <- fn_for_ds_test_2
# test methods
ds_test_methods <- c("parametric", "nonparametric", "adaptive")
# run test
sim_output <- perform_ds_func(
sample_sizes       = sizes,
Nsim               = Nsim,
alpha              = alpha,
gen_data           = gen_data,
get_parameters     = get_parameters,
fn_to_get_norm_obj = fn_get_norm_obj,
fn_for_norm_test   = normality_test,
fn_for_ds_test_1   = fn_ds_test_1,
fn_for_ds_test_2   = fn_ds_test_2,
norm_test_method   = norm_test,
ds_test_methods    = ds_test_methods,
effect_size        = effect_size
)
# Extract results and timing
ds_test_results <- sim_output$results
timing_results <- sim_output$timing
# Access individual test results
ds_test1_results  <- ds_test_results$parametric
ds_test2_results <- ds_test_results$nonparametric
ds_adaptive_results  <- ds_test_results$adaptive
auc_df <- data.frame(
Method = c("Test 1(t_test)", "Test 2(Wilcoxon)", "Test 3(Adaptive)"),
AUC = c(
compute_area(sizes, ds_test1_results),
compute_area(sizes, ds_test2_results),
compute_area(sizes, ds_adaptive_results)
),
Runtime = c(
timing_results$parametric,
timing_results$nonparametric,
timing_results$adaptive
)
)
results <- list(
sizes = sizes,
ds_test1_results = ds_test1_results,
ds_test2_results = ds_test2_results,
ds_adaptive_results = ds_adaptive_results,
auc = auc_df,
timing = timing_results
)
auc_df
