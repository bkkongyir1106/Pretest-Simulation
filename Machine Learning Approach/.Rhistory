# Convenience aliases
pow_t     <- power_results$parametric
pow_w     <- power_results$nonparametric
pow_adapt <- power_results$adaptive
err_t     <- type1_results$parametric
err_w     <- type1_results$nonparametric
err_adapt <- type1_results$adaptive
# Optional AUC for power curves
compute_area <- function(sample_sizes, y) {
if (is.null(y) || length(y) != length(sample_sizes)) return(NA_real_)
sum(diff(sample_sizes) * (head(y, -1) + tail(y, -1)) / 2) /
(max(sample_sizes) - min(sample_sizes))
}
# Curves (from your previous run)
pow_curves <- list(
Parametric    = power_results$parametric,
Nonparametric = power_results$nonparametric,
Adaptive      = power_results$adaptive
)
err_curves <- list(
Parametric    = type1_results$parametric,
Nonparametric = type1_results$nonparametric,
Adaptive      = type1_results$adaptive
)
# AUCs
auc_power <- sapply(pow_curves, function(y) compute_area(sizes, y))
auc_type1 <- sapply(err_curves, function(y) compute_area(sizes, y))
auc_table <- data.frame(
Method    = names(auc_power),
AUC_Power = unname(auc_power),
AUC_TypeI = unname(auc_type1),
row.names = NULL
)
print(auc_table, digits = 4)
# ------------------------------------------------------------------------------
# ----------------- Plot Power/Type I error -----------------------
pdf("reg_test_power_and_typeI.pdf", width = 12, height = 6)
op <- par(no.readonly = TRUE); on.exit(try(par(op), silent = TRUE), add = TRUE)
par(mfrow = c(1, 2), mar = c(4.2, 4.5, 3, 1))
# Power panel (left)
plot(sizes, pow_curves$Parametric, type="b", pch=19, col="red", lwd=2,
ylim=c(0,1), xlab="Sample Size (per group)", ylab="Power",
main="Power vs Sample Size(Exp)")
lines(sizes, pow_curves$Nonparametric, type="b", pch=17, col="blue",  lwd=2)
lines(sizes, pow_curves$Adaptive,      type="b", pch=15, col="green", lwd=2)
abline(h = 0.8, col = "gray50", lty = 2, lwd = 1.5)
legend("bottomright",
legend = paste0(c("Parametric","Nonparametric","Adaptive"),
" (AUC=", formatC(auc_power[c("Parametric","Nonparametric","Adaptive")],
format="f", digits=3), ")"),
col = c("red","blue","green"), pch = c(19,17,15), lty = 1, lwd = 2, bty = "b")
# Type I error panel (right)
plot(sizes, err_curves$Parametric, type="b", pch=19, col="red", lwd=2,
ylim=c(0, max(0.12, err_curves$Parametric, err_curves$Nonparametric, err_curves$Adaptive, na.rm=TRUE)),
xlab="Sample Size (per group)", ylab="Type I Error",
main="Type I Error vs Sample Size(Exp)")
lines(sizes, err_curves$Nonparametric, type="b", pch=17, col="blue",  lwd=2)
lines(sizes, err_curves$Adaptive,      type="b", pch=15, col="green", lwd=2)
abline(h = alpha, col = "gray40", lty = 3, lwd = 1.5)
legend("topleft",
legend = paste0(c("Parametric","Nonparametric","Adaptive"),
" (AUC=", formatC(auc_type1[c("Parametric","Nonparametric","Adaptive")],
format="f", digits=3), ")"),
col = c("red","blue","green"), pch = c(19,17,15), lty = 1, lwd = 2, bty = "b")
dev.off()
# =============================================================================
# -----------------------------------------------------------------------------
# Function to compute FPR and TPR for Normality Test Methods
# -----------------------------------------------------------------------------
fn_for_roc_curve_for_norm_test <- function(n, alpha_pretest, H1_dist, tests, Nsim = 1e3) {
FPR <- TPR <- matrix(0, nrow = length(tests), ncol = length(alpha_pretest))
rownames(FPR) <- rownames(TPR) <- tests
colnames(FPR) <- colnames(TPR) <- paste0("alpha_", alpha_pretest)
pb <- txtProgressBar(min = 0, max = Nsim * length(tests) * length(alpha_pretest), style = 3)
counter <- 0
for (i in seq_along(tests)) {
test_name <- tests[i]
for (j in seq_along(alpha_pretest)) {
alpha <- alpha_pretest[j]
reject_H0 <- reject_H1 <- numeric(Nsim)
for (k in 1:Nsim) {
# Under H0
paras_H0 <- get_parameters(n, dist = "Normal", par = NULL)
normal_data <- do.call(gen_data, paras_H0)
# Under H1
paras_H1 <- get_parameters(n, dist = H1_dist)
non_normal_data <- do.call(gen_data, paras_H1)
# Get normality test object
normal_data <- fn_to_get_norm_obj(normal_data)
non_normal_data <- fn_to_get_norm_obj(non_normal_data)
# perform normality test for H0 and H1
pvals_H0 <- normality_test(normal_data, test = test_name, alpha = alpha)$p_values
reject_H0[k] <- any(pvals_H0 < alpha, na.rm = TRUE)
pvals_H1 <- normality_test(non_normal_data, test = test_name, alpha = alpha)$p_values
reject_H1[k] <- any(pvals_H1 < alpha, na.rm = TRUE)
counter <- counter + 1
setTxtProgressBar(pb, counter)
}
# calculate FPR & TPR
FPR[i, j] <- mean(reject_H0, na.rm = TRUE)
TPR[i, j] <- mean(reject_H1, na.rm = TRUE)
}
}
close(pb)
return(list(
pvals_H0 = pvals_H0,
pvals_H1 = pvals_H1,
FPR = FPR,
TPR = TPR,
alpha = alpha_pretest))
}
# --------------- Run the simulation -------------------
Nsim = 1e4
alpha_pretest = seq(from = 0.0, to = 1, by = 0.05)
roc_pval_ds_test <- fn_for_roc_curve_for_norm_test(
n = 10,
alpha_pretest = alpha_pretest,
H1_dist = "exponential",
tests = c("SW", "SF", "JB","SKEW") #, "DAP", "AD", "CVM"),
,
Nsim = Nsim
)
# ------------------------------------------------------------------------------
#              Function to plot ROC curves from FPR and TPR matrices
# ------------------------------------------------------------------------------
roc_curve_for_norm_test <- function(FPR, TPR, tests_to_plot = rownames(FPR), alpha = NULL,
title = "ROC Curves for Different Normality Tests(Chisq_3)") {
# define line colors and plot characters
colors <- 1:length(tests_to_plot)
#colors <- rainbow(length(tests_to_plot))
plot_chars <- 1:length(tests_to_plot) + 14
# create an empty plot
plot(0, 0, type = "n",
xlim = c(0, 1),
ylim = c(0, 1),
xlab = "False Positive Rate (FPR)",
ylab = "True Positive Rate (TPR)",
main = title)
# add lines for each test
for (i in seq_along(tests_to_plot)) {
test <- tests_to_plot[i]
lines(FPR[test, ], TPR[test, ], col = colors[i], lwd = 2)
points(FPR[test, ], TPR[test, ], col = colors[i], pch = plot_chars[i], cex = 0.5)
}
# add reference line
abline(0, 1, lty = 2, col = "gray")
legend("bottomright",
legend = tests_to_plot,
col = colors,
pch = plot_chars,
lwd = 2,
title = "Normality Tests")
}
# Plot ROC using selected tests
pdf("reg_test_norm_roc_curve.pdf", width=12, height=8)
selected_tests <- c("SW", "SF", "JB", "SKEW")
roc_curve_for_norm_test(FPR = roc_pval_ds_test$FPR,
TPR = roc_pval_ds_test$TPR,
tests_to_plot = selected_tests,
alpha = roc_pval_ds_test$alpha)
dev.off()
# =============================================================================
# -----------------------------------------------------------------------------
# Function to compute p-values for normality test & downstream tests
# -----------------------------------------------------------------------------
generate_pval<- function(N, n, effect_size, test = "SW", dist = "Normal", ...) {
# Initialize storage
pval_t.test_H0 <- pval_wilcox.test_H0 <- numeric(N)
pval_t.test_H1 <- pval_wilcox.test_H1 <- numeric(N)
norm_pvals_H0 <- norm_pvals_H1 <- vector("list", N)
pb <- txtProgressBar(min = 0, max = N, style = 3)
for (i in 1:N) {
# Under null hypothesis
paras_H0 <- get_parameters(n, dist = dist, effect_size = c(0.0, 0.0, 0.0, 0.0, 0.0))
data_H0 <- do.call(gen_data, paras_H0)
# Under alternative hypothesis
paras_H1 <- get_parameters(n, dist = dist, effect_size = effect_size)
data_H1 <- do.call(gen_data, paras_H1)
# Get normality test objects
normality_test_object_H0 <- fn_to_get_norm_obj(data_H0)
normality_test_object_H1 <- fn_to_get_norm_obj(data_H1)
# perform normality test
normality_test_H0 <- normality_test(normality_test_object_H0, test = test, alpha = 0.05)
normality_test_H1 <- normality_test(normality_test_object_H1, test = test, alpha = 0.05)
# Store normality p-values
norm_pvals_H0[[i]] <- normality_test_H0$p_values
norm_pvals_H1[[i]] <- normality_test_H1$p_values
# Get test p-values under null and alternative
pval_t.test_H0[i] <- fn_for_ds_test_1(data_H0)$p.value
pval_wilcox.test_H0[i] <- fn_for_ds_test_2(data_H0)$p.value
pval_t.test_H1[i] <- fn_for_ds_test_1(data_H1)$p.value
pval_wilcox.test_H1[i] <- fn_for_ds_test_2(data_H1)$p.value
setTxtProgressBar(pb, i)
}
close(pb)
return(list(
pval_t.test_H0 = pval_t.test_H0,
pval_wilcox.test_H0 = pval_wilcox.test_H0,
pval_t.test_H1 = pval_t.test_H1,
pval_wilcox.test_H1 = pval_wilcox.test_H1,
norm_pvals_H0 = norm_pvals_H0,
norm_pvals_H1 = norm_pvals_H1
))
}
# --------------- Calculate power/Type I errors for each test method --------------
perform_analysis <- function(N, n, distributions, effect_size, test, alpha_pretest, test_alpha) {
ds_test_results <- list()
error_ds_test <- list()
power_ds_test <- list()
pb_dist <- txtProgressBar(min = 0, max = length(distributions), style = 3)
for(dist_idx in seq_along(distributions)) {
dist <- distributions[dist_idx]
cat("Processing distribution:", dist, "\n")
# Store all p-values from generate_pval
ds_test_results[[dist]] <- generate_pval(N, n, effect_size = effect_size, test = "SW" , dist = dist)
# Calculate Type I error rates (under H0)
error_ds_test[[dist]] <- list(
error_t.test = mean(ds_test_results[[dist]]$pval_t.test_H0 < test_alpha),
error_wilcox.test = mean(ds_test_results[[dist]]$pval_wilcox.test_H0 < test_alpha),
# storage for adaptive test for error
adaptive_wilcox_error = numeric(length(alpha_pretest)))
# Calculate Power (under H1)
power_ds_test[[dist]] <- list(
power_t.test = mean(ds_test_results[[dist]]$pval_t.test_H1 < test_alpha),
power_wilcox.test = mean(ds_test_results[[dist]]$pval_wilcox.test_H1 < test_alpha),
# storage for adaptive test for power
adaptive_wilcox_power = numeric(length(alpha_pretest)))
pb_alpha <- txtProgressBar(min = 0, max = length(alpha_pretest), style = 3)
# compute decisions for each alpha level
for(j in seq_along(alpha_pretest)) {
alpha <- alpha_pretest[j]
# For Type I error (H0)
use_t_test_H0 <- sapply(ds_test_results[[dist]]$norm_pvals_H0, function(x) all(x > alpha))
adaptive_pvals_H0 <- ifelse(use_t_test_H0,
ds_test_results[[dist]]$pval_t.test_H0,
ds_test_results[[dist]]$pval_wilcox.test_H0)
error_ds_test[[dist]]$adaptive_wilcox_error[j] <- mean(adaptive_pvals_H0 < test_alpha)
# For Power (H1)
use_t_test_H1 <- sapply(ds_test_results[[dist]]$norm_pvals_H1, function(x) all(x > alpha))
adaptive_pvals_H1 <- ifelse(use_t_test_H1,
ds_test_results[[dist]]$pval_t.test_H1,
ds_test_results[[dist]]$pval_wilcox.test_H1)
power_ds_test[[dist]]$adaptive_wilcox_power[j] <- mean(adaptive_pvals_H1 < test_alpha)
setTxtProgressBar(pb_alpha, j)
}
close(pb_alpha)
setTxtProgressBar(pb_dist, dist_idx)
}
close(pb_dist)
return(list(
error_ds_test = error_ds_test,
power_ds_test = power_ds_test,
all_pvalues     = ds_test_results   # raw p-values saved here
))
}
# run analysis to get power and error
Nsim <- 1e6
alpha_pretest = seq(from = 0.005, to = 1, by = 0.005)
#alpha_pretest = seq(from = 0.009, to = 1, by = 0.0025)
analysis_ds_tests <- perform_analysis(
N = Nsim,
n = 10,
distributions = c("Normal", "exponential"),
effect_size = effect_size,
test = "SW",
alpha_pretest = alpha_pretest,
test_alpha = 0.05
)
setwd("/Users/benedictkongyir/Desktop/OSU/Fall2025/Computational Statistics/Homework")
# Load data
Y<-read.table("ON100.txt", header = T, row.names = 1)
# transpose Y
Y <- t(Y)
# subset Y to only the 100th bacteria
y <- Y[, 100]
x<-c(8.26,9.75,10.26,10.62, 10.82, 11.17, 11.49, 11.73,12.23)
# problem 2
# simple linear regression model
my.lm <- lm(y ~x)
# confidence intervals
lm.ci <- confint(my.lm)
# problem 3
my.glm <- glm(y ~ x, family = poisson)
glm.ci <- confint(my.glm)
coef(my.glm)
####### Problem #3 ##################
# Load data (adjust path as needed)
setwd("/Users/benedictkongyir/Desktop/OSU/Fall2025/Computational Statistics/Homework")
Y <- read.table("ON100.txt", header = TRUE, row.names = 1)
Y <- t(Y) # Transpose so rows are samples, columns are bacteria
y <- Y[, 100] # Extract the 100th bacteria's counts
# Known predictor values (concentration)
x <- c(8.26, 9.75, 10.26, 10.62, 10.82, 11.17, 11.49, 11.73, 12.23)
# Fit the Poisson GLM
model <- glm(y ~ x, family = poisson(link = "log"))
# Get the MLEs
beta_hat <- coef(model)
beta_0_hat <- beta_hat[1] # Estimate for β₀
beta_1_hat <- beta_hat[2] # Estimate for β₁
# Print the results
print(summary(model))
cat("\nMaximum Likelihood Estimates:\n")
cat("β₀_hat =", round(beta_0_hat, 4), "\n")
cat("β₁_hat =", round(beta_1_hat, 4), "\n")
## part c
# From the fitted model in part (b)
theta_hat <- exp(beta_0_hat)
var_beta0 <- vcov(model)[1, 1] # Variance of β₀_hat
var_theta <- (exp(beta_0_hat)^2) * var_beta0
se_theta <- sqrt(var_theta)
cat("Estimate of θ = E[Y|X=0]:", round(theta_hat, 4), "\n")
cat("Standard Error for θ_hat:", round(se_theta, 4), "\n")
vcov(model)
vcov(model)[1, 1]
(exp(beta_0_hat)^2) * var_beta0
sqrt(var_theta)
exp(beta_0_hat)
(exp(beta_0_hat)^2) * var_beta0
(exp(beta_0_hat)^2) * var_beta0
setwd("~/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/NORMALITY TEST METHODS/ROC_Normality.test.RData")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach/trained_models.RData")
plot_ml_vs_sw_roc <- function(eval_results,
FPR_list, TPR_list,
sw_key = "SW",
legend_loc = "bottomright",
file = "SW_test_ML_models.pdf") {
# Optional PDF export
if (!is.null(file)) {
pdf(file, width = 8, height = 6)
on.exit(dev.off(), add = TRUE)
}
# compute AUC from an ROC curve (FPR, TPR) via trapezoids ---
roc_auc_from_curve <- function(fpr, tpr) {
ok <- is.finite(fpr) & is.finite(tpr)
fpr <- fpr[ok]
tpr <- tpr[ok]
ord <- order(fpr)
f <- fpr[ord]
t <- tpr[ord]
# ensure endpoints are present
if (length(f) == 0) return(NA_real_)
if (f[1] > 0 || t[1] > 0) { f <- c(0, f); t <- c(0, t) }
if (tail(f, 1) < 1 || tail(t, 1) < 1) { f <- c(f, 1); t <- c(t, 1) }
sum(diff(f) * (head(t, -1) + tail(t, -1)) / 2)
}
# Get SW curve from lists
if (!sw_key %in% names(FPR_list) || !sw_key %in% names(TPR_list)) {
stop(sprintf("Couldn't find '%s' in FPR_list/TPR_list.", sw_key))
}
sw_fpr <- as.numeric(FPR_list[[sw_key]])
sw_tpr <- as.numeric(TPR_list[[sw_key]])
sw_auc <- roc_auc_from_curve(sw_fpr, sw_tpr)
# Prepare ML model names
drop_names <- c("Shapiro-Wilk", "SW", "KS", "AD", "DAP", "SF", "JB", "CVM")
ml_names <- setdiff(names(eval_results), drop_names)
if (length(ml_names) == 0) {
warning("No ML models detected in eval_results after filtering; plotting SW only.")
}
# Base frame ---
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "False Positive Rate (1 - Specificity)",
ylab = "True Positive Rate (Sensitivity)",
main = "ROC: Shapiro–Wilk vs. Machine Learning (Positive: Non_Normal)")
abline(a = 0, b = 1, col = "gray", lty = 2)
grid(col = "lightgray", lty = "dotted")
# --- Plot ML curves ---
if (length(ml_names) > 0) {
cols <- setNames(rainbow(length(ml_names)), ml_names)
legend_labels <- character(0)
legend_cols   <- character(0)
legend_lty    <- integer(0)
legend_lwd    <- numeric(0)
for (model_name in ml_names) {
pred_df <- eval_results[[model_name]]$Predictions
if (!("Prob_Non_Normal" %in% colnames(pred_df))) next
y_true <- ifelse(pred_df$True_Class == "Non_Normal", 1, 0)
scores <- pred_df$Prob_Non_Normal
keep <- is.finite(scores) & !is.na(scores)
if (!any(keep)) next
roc_obj <- pROC::roc(y_true[keep], scores[keep], levels = c(0, 1),
direction = "<", quiet = TRUE)
auc_val <- as.numeric(round(pROC::auc(roc_obj), 3))
lines(1 - roc_obj$specificities, roc_obj$sensitivities,
col = cols[model_name], lty = 1, lwd = 2)
legend_labels <- c(legend_labels, sprintf("%s (AUC=%.3f)", model_name, auc_val))
legend_cols   <- c(legend_cols, cols[model_name])
legend_lty    <- c(legend_lty, 1)
legend_lwd    <- c(legend_lwd, 2)
}
} else {
legend_labels <- legend_cols <- character(0)
legend_lty <- legend_lwd <- numeric(0)
}
# --- Plot SW curve (black dashed) ---
lines(sw_fpr, sw_tpr, col = "black", lwd = 3, lty = 2)
sw_label <- sprintf("Shapiro–Wilk (AUC=%.3f)", sw_auc)
# --- Legend (SW + ML) ---
legend(legend_loc,
legend = c(sw_label, legend_labels),
col    = c("black", legend_cols),
lty    = c(2, legend_lty),
lwd    = c(3, legend_lwd),
bty    = "o", cex = 0.9,
title  = "Models / Test")
}
plot_ml_vs_sw_roc(eval_results, FPR_list, TPR_list, sw_key = "SW")
# Optional to save as PDF:
plot_ml_vs_sw_roc(eval_results, FPR_list, TPR_list, sw_key = "SW",
file = "ROC_SW_vs_ML.pdf")
setwd("~/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/NORMALITY TEST METHODS/ROC_Normality.test.RData")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach/trained_models.RData")
setwd("~/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/NORMALITY TEST METHODS/ROC_Normality.test.RData")
setwd("~/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/NORMALITY TEST METHODS/ROC_Normality.test.RData")
load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Machine Learning Approach/trained_models.RData")
plot_ml_vs_sw_roc <- function(eval_results,
FPR_list, TPR_list,
sw_key = "SW",
legend_loc = "bottomright",
file = "SW_test_ML_models.pdf") {
# Optional PDF export
if (!is.null(file)) {
pdf(file, width = 8, height = 6)
on.exit(dev.off(), add = TRUE)
}
# compute AUC from an ROC curve (FPR, TPR) via trapezoids ---
roc_auc_from_curve <- function(fpr, tpr) {
ok <- is.finite(fpr) & is.finite(tpr)
fpr <- fpr[ok]
tpr <- tpr[ok]
ord <- order(fpr)
f <- fpr[ord]
t <- tpr[ord]
# ensure endpoints are present
if (length(f) == 0) return(NA_real_)
if (f[1] > 0 || t[1] > 0) { f <- c(0, f); t <- c(0, t) }
if (tail(f, 1) < 1 || tail(t, 1) < 1) { f <- c(f, 1); t <- c(t, 1) }
sum(diff(f) * (head(t, -1) + tail(t, -1)) / 2)
}
# --- Get SW curve from lists ---
if (!sw_key %in% names(FPR_list) || !sw_key %in% names(TPR_list)) {
stop(sprintf("Couldn't find '%s' in FPR_list/TPR_list.", sw_key))
}
sw_fpr <- as.numeric(FPR_list[[sw_key]])
sw_tpr <- as.numeric(TPR_list[[sw_key]])
sw_auc <- roc_auc_from_curve(sw_fpr, sw_tpr)
# --- Prepare ML model names (exclude any normality-test placeholders) ---
drop_names <- c("Shapiro-Wilk", "SW", "KS", "AD", "DAP", "SF", "JB", "CVM")
ml_names <- setdiff(names(eval_results), drop_names)
if (length(ml_names) == 0) {
warning("No ML models detected in eval_results after filtering; plotting SW only.")
}
# --- Base frame ---
plot(0, 0, type = "n", xlim = c(0, 1), ylim = c(0, 1),
xlab = "False Positive Rate (1 - Specificity)",
ylab = "True Positive Rate (Sensitivity)",
main = "ROC: Shapiro–Wilk vs. Machine Learning (Positive: Non_Normal)")
abline(a = 0, b = 1, col = "gray", lty = 2)
grid(col = "lightgray", lty = "dotted")
# --- Plot ML curves ---
if (length(ml_names) > 0) {
cols <- setNames(rainbow(length(ml_names)), ml_names)
legend_labels <- character(0)
legend_cols   <- character(0)
legend_lty    <- integer(0)
legend_lwd    <- numeric(0)
for (model_name in ml_names) {
pred_df <- eval_results[[model_name]]$Predictions
if (!("Prob_Non_Normal" %in% colnames(pred_df))) next
y_true <- ifelse(pred_df$True_Class == "Non_Normal", 1, 0)
scores <- pred_df$Prob_Non_Normal
keep <- is.finite(scores) & !is.na(scores)
if (!any(keep)) next
roc_obj <- pROC::roc(y_true[keep], scores[keep], levels = c(0, 1),
direction = "<", quiet = TRUE)
auc_val <- as.numeric(round(pROC::auc(roc_obj), 3))
lines(1 - roc_obj$specificities, roc_obj$sensitivities,
col = cols[model_name], lty = 1, lwd = 2)
legend_labels <- c(legend_labels, sprintf("%s (AUC=%.3f)", model_name, auc_val))
legend_cols   <- c(legend_cols, cols[model_name])
legend_lty    <- c(legend_lty, 1)
legend_lwd    <- c(legend_lwd, 2)
}
} else {
legend_labels <- legend_cols <- character(0)
legend_lty <- legend_lwd <- numeric(0)
}
# --- Plot SW curve (black dashed) ---
lines(sw_fpr, sw_tpr, col = "black", lwd = 3, lty = 2)
sw_label <- sprintf("Shapiro–Wilk (AUC=%.3f)", sw_auc)
# --- Legend (SW + ML) ---
legend(legend_loc,
legend = c(sw_label, legend_labels),
col    = c("black", legend_cols),
lty    = c(2, legend_lty),
lwd    = c(3, legend_lwd),
bty    = "o", cex = 0.9,
title  = "Models / Test")
}
## ---- Usage ---------------------------------------------------------------
## 1) Make sure you've run your ML pipeline and have `eval_results`
## 2) Load the ROC points for tests (already in your snippet):
##    load("/Users/benedictkongyir/Desktop/OSU/Research/Pretest-Simulation/Simulation/NORMALITY TEST METHODS/ROC_Normality.test.RData")
##    (provides FPR_list, TPR_list with an entry "SW")
## 3) Call:
plot_ml_vs_sw_roc(eval_results, FPR_list, TPR_list, sw_key = "SW")
# Optional to save as PDF:
plot_ml_vs_sw_roc(eval_results, FPR_list, TPR_list, sw_key = "SW",
file = "ROC_SW_vs_ML.pdf")
